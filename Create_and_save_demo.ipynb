{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fluderm/PyBulletPerAct/blob/main/Create_and_save_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNovH9jQPdTJ"
      },
      "source": [
        "# **Save demo from Panda (compatible with RLBench and PyBulletPerAct):** \n",
        "\n",
        "In this notebook we implement methods to save PyBullet/panda_gym simulation demos in accordance with [`YARR`](https://github.com/stepjam/YARR), [`RLBench`](https://github.com/stepjam/RLBench) and importantly in accordance with [PyBulletPerAct](https://github.com/fellowship/Robotics-Simulator-Imitation-Learning/blob/main/PyBulletPerAct.ipynb).\n",
        "\n",
        "You should be able to generate simple demos for various tasks and save them in such a way that it feeds straightforwardly into the PyBulletPerAct training pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY8iX-eLYepQ"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kb4kpb-wtLV"
      },
      "outputs": [],
      "source": [
        "%pip install setuptools==65.5.0 # Need to add this for panda-gym==2.0.0 to work\n",
        "!apt install -y python-opengl ffmpeg > /dev/null 2>&1\n",
        "%pip install pyvirtualdisplay\n",
        "\n",
        "#stable_baselines3 sb3_contrib panda-gym==2.0.0 \n",
        "!pip install pyvirtualdisplay \n",
        "!apt install xvfb python-opengl ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGIz2RohYkJ9"
      },
      "source": [
        "### Clone Repo and Setup\n",
        "\n",
        "This repo contains barebones code from [`panda_gym`](https://github.com/qgallouedec/panda-gym).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fluderm/PyBulletPerAct.git\n",
        "%pip install -e PyBulletPerAct\n",
        "\n",
        "# copy glass files to current directory:\n",
        "\n",
        "!cp PyBulletPerAct/panda_gym/plastic_coffee_cup.mtl \\\n",
        " PyBulletPerAct/panda_gym/plastic_coffee_cup_vhacd.obj \\\n",
        " PyBulletPerAct/panda_gym/plastic_coffee_cup.obj ."
      ],
      "metadata": {
        "id": "vbbOjnVUeZMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12mooJuWggGm"
      },
      "source": [
        "# Panda Gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('PyBulletPerAct')\n"
      ],
      "metadata": {
        "id": "Rsd5DjSYhTPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6T-n2Cpmxo6"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import panda_gym\n",
        "import pprint\n",
        "import numpy as np\n",
        "import pybullet as p\n",
        "import math\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "from matplotlib import pyplot as plt, animation\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "'''\n",
        "The following are some basic functions that help render the virtual PyBullet\n",
        "environment:\n",
        "'''\n",
        "\n",
        "\n",
        "def create_anim(frames, dpi, fps):\n",
        "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    def setup():\n",
        "        plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n",
        "    return anim\n",
        "\n",
        "def display_anim(frames, dpi=72, fps=50):\n",
        "    anim = create_anim(frames, dpi, fps)\n",
        "    return anim.to_jshtml()\n",
        "\n",
        "def save_anim(frames, filename, dpi=72, fps=50):\n",
        "    anim = create_anim(frames, dpi, fps)\n",
        "    anim.save(filename)\n",
        "\n",
        "\n",
        "class trigger:\n",
        "    def __init__(self):\n",
        "        self._trigger = True\n",
        "\n",
        "    def __call__(self, e):\n",
        "        return self._trigger\n",
        "\n",
        "    def set(self, t):\n",
        "        self._trigger = t"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick test if everything works:\n",
        "\n",
        "The relevant environment is stored under PandaPickAndPlace-v2. One can of course rename this environment (need to register inside panda_gym '__init__' file)."
      ],
      "metadata": {
        "id": "ckyqCC44hb5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#env.close()\n",
        "env = gym.make('PandaPickAndPlace-v2',control_type='joints')\n",
        "obs = env.reset()\n",
        "env.robot.neutral_joint_values = np.array([0.00, 0.41, 0.00, -1.85, 3.14, 2.52, 0.79, 3.00, 3.00])\n",
        "print(obs)\n",
        "\n",
        "rgb, depth, mask, misc = env.render(mode = 'rgb_array')\n",
        "display.HTML(display_anim([rgb['front']]))"
      ],
      "metadata": {
        "id": "pxaoTQC7hiWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB-ans9bgoUm"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "\n",
        "# enable joint force sensor:\n",
        "[(env.sim.physics_client.enableJointForceTorqueSensor(0,i)) for i in range(11)];"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ewRaaU31Ri"
      },
      "source": [
        "# Save panda dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgSmROJFulWY"
      },
      "source": [
        "## Some Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avjXcUrtfztn"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "from time import sleep\n",
        "\n",
        "def no_disconnect():\n",
        "  '''\n",
        "  function that ensures Colab pro doesn't disconnect.\n",
        "  '''\n",
        "  t = dt.datetime.now()\n",
        "\n",
        "  while True:\n",
        "      delta = dt.datetime.now()-t               \n",
        "      if delta.seconds >= 60:\n",
        "          print(\"1 Min\",dt.datetime.now())\n",
        "          t = dt.datetime.now()\n",
        "      sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS34KOF_R-ap"
      },
      "outputs": [],
      "source": [
        "def _normalize(pc):\n",
        "  return (pc - np.amin(pc)) / (np.amax(pc) - np.amin(pc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2YbnoQLIQ8G"
      },
      "outputs": [],
      "source": [
        "#from multiprocessing import Process, Manager\n",
        "import sys\n",
        "\n",
        "sys.path.append('peract_colab')\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "#from absl import app\n",
        "#from absl import flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMQE2zRB33Ez"
      },
      "outputs": [],
      "source": [
        "panda_dataset_dir = 'PyBulletPerAct/data'\n",
        "\n",
        "def check_and_make(dir):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "check_and_make(panda_dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqkP22_rKoSL"
      },
      "outputs": [],
      "source": [
        "def ClipFloatValues(float_array, min_value, max_value):\n",
        "  '''\n",
        "  Clips values to the range [min_value, max_value].\n",
        "  First checks if any values are out of range and prints a message.\n",
        "  Then clips all values to the given range.\n",
        "  Args:\n",
        "    float_array: 2D array of floating point values to be clipped.\n",
        "    min_value: Minimum value of clip range.\n",
        "    max_value: Maximum value of clip range.\n",
        "  Returns:\n",
        "    The clipped array.\n",
        "  '''\n",
        "  if float_array.min() < min_value or float_array.max() > max_value:\n",
        "    float_array = np.clip(float_array, min_value, max_value)\n",
        "  return float_array\n",
        "\n",
        "DEFAULT_RGB_SCALE_FACTOR = 256000.0\n",
        "\n",
        "def float_array_to_rgb_image(float_array,\n",
        "                             scale_factor=DEFAULT_RGB_SCALE_FACTOR,\n",
        "                             drop_blue=False):\n",
        "  '''\n",
        "  Convert a floating point array of values to an RGB image.\n",
        "  Convert floating point values to a fixed point representation where\n",
        "  the RGB bytes represent a 24-bit integer.\n",
        "  R is the high order byte.\n",
        "  B is the low order byte.\n",
        "  The precision of the depth image is 1/256 mm.\n",
        "  Floating point values are scaled so that the integer values cover\n",
        "  the representable range of depths.\n",
        "  This image representation should only use lossless compression.\n",
        "  Args:\n",
        "    float_array: Input array of floating point depth values in meters.\n",
        "    scale_factor: Scale value applied to all float values.\n",
        "    drop_blue: Zero out the blue channel to improve compression, results in 1mm\n",
        "      precision depth values.\n",
        "  Returns:\n",
        "    24-bit RGB PIL Image object representing depth values.\n",
        "  '''\n",
        "  # Scale the floating point array.\n",
        "  scaled_array = np.floor(float_array * scale_factor + 0.5)\n",
        "\n",
        "  # Convert the array to integer type and clip to representable range.\n",
        "  min_inttype = 0\n",
        "  max_inttype = 2**24 - 1\n",
        "  scaled_array = ClipFloatValues(scaled_array, min_inttype, max_inttype)\n",
        "  int_array = scaled_array.astype(np.uint32)\n",
        "  # Calculate:\n",
        "  #   r = (f / 256) / 256  high byte\n",
        "  #   g = (f / 256) % 256  middle byte\n",
        "  #   b = f % 256          low byte\n",
        "  rg = np.divide(int_array, 256)\n",
        "  r = np.divide(rg, 256)\n",
        "  g = np.mod(rg, 256)\n",
        "  image_shape = int_array.shape\n",
        "  rgb_array = np.zeros((image_shape[0], image_shape[1], 3), dtype=np.uint8)\n",
        "  rgb_array[..., 0] = r\n",
        "  rgb_array[..., 1] = g\n",
        "  if not drop_blue:\n",
        "    # Calculate the blue channel and add it to the array.\n",
        "    b = np.mod(int_array, 256)\n",
        "    rgb_array[..., 2] = b\n",
        "  image_mode = 'RGB'\n",
        "  image = Image.fromarray(rgb_array, mode=image_mode)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-xUNx-D8JJP"
      },
      "outputs": [],
      "source": [
        "# Within PyBullet can also do p.getMatrixFromQuaternion(com_o)\n",
        "\n",
        "def rotationMatrixToEulerAngles(R) :\n",
        "    '''\n",
        "    function that transforms rotation 3x3 matrix to Euler angles:\n",
        "    '''\n",
        "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
        "    singular = sy < 1e-6\n",
        "    if  not singular :\n",
        "        x = math.atan2(R[2,1] , R[2,2])\n",
        "        y = math.atan2(-R[2,0], sy)\n",
        "        z = math.atan2(R[1,0], R[0,0])\n",
        "    else :\n",
        "        x = math.atan2(-R[1,2], R[1,1])\n",
        "        y = math.atan2(-R[2,0], sy)\n",
        "        z = 0\n",
        " \n",
        "    return np.array([x, y, z])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFu7zoy5Eck6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.transform import Rotation as Rot\n",
        "\n",
        "def cvK2BulletP(K, w, h, near, far):\n",
        "    '''\n",
        "    cvKtoPulletP converts the K interinsic matrix as calibrated using Opencv\n",
        "    and ROS to the projection matrix used in openGL and Pybullet.\n",
        "\n",
        "    :param K:  OpenCV 3x3 camera intrinsic matrix\n",
        "    :param w:  Image width\n",
        "    :param h:  Image height\n",
        "    :near:     The nearest objects to be included in the render\n",
        "    :far:      The furthest objects to be included in the render\n",
        "    :return:   4x4 projection matrix as used in openGL and pybullet\n",
        "    '''\n",
        "    f_x = K[0,0]\n",
        "    f_y = K[1,1]\n",
        "    c_x = K[0,2]\n",
        "    c_y = K[1,2]\n",
        "    A = (near + far)/(near - far)\n",
        "    B = 2 * near * far / (near - far)\n",
        "\n",
        "    projection_matrix = [\n",
        "                        [2/w * f_x,  0,          (w - 2*c_x)/w,  0],\n",
        "                        [0,          2/h * f_y,  (2*c_y - h)/h,  0],\n",
        "                        [0,          0,          A,              B],\n",
        "                        [0,          0,          -1,             0]]\n",
        "    # The transpose is needed for respecting the array structure of the PyB\n",
        "    # and OpenGL\n",
        "    return np.array(projection_matrix).T.reshape(16).tolist()\n",
        "\n",
        "def BulletP2cvK(proj_mat, w, h):\n",
        "    '''\n",
        "    inverse of above cvK2BulletP function\n",
        "    '''\n",
        "    A,B = proj_mat[2,2],proj_mat[3,2]\n",
        "    n,f = B/(A-1), B/(A+1)\n",
        "    fx,fy = w*proj_mat[0,0]/2,h*proj_mat[1,1]/2\n",
        "    cx,cy = w*(1-proj_mat[2,0])/2, h*(proj_mat[2,1]+1)/2\n",
        "\n",
        "    return np.array([[fx,0,cx],[0,fy,cy],[0,0,1]]), n, f\n",
        "\n",
        "\n",
        "def cvPose2BulletView(Rt):\n",
        "    '''\n",
        "    cvPose2BulletView gets orientation and position as used \n",
        "    in ROS-TF and opencv and coverts it to the view matrix used \n",
        "    in openGL and pyBullet.\n",
        "    \n",
        "    :param Rt: 4x4 extrinsic matrix\n",
        "    :return:  4x4 view matrix as used in PyBullet and openGL\n",
        "    \n",
        "    '''\n",
        "\n",
        "    Tc = np.array([[1,   0,    0,  0],\n",
        "                   [0,  -1,    0,  0],\n",
        "                   [0,   0,   -1,  0],\n",
        "                   [0,   0,    0,  1]]).reshape(4,4)\n",
        "    \n",
        "    # pybullet pse is the inverse of the pose from the ROS-TF\n",
        "    T=Tc@np.linalg.inv(Rt)\n",
        "    # The transpose is needed for respecting the array structure of the OpenGL\n",
        "    viewMatrix = T.T.reshape(16)\n",
        "    return viewMatrix\n",
        "\n",
        "def BullettocvPose(X):\n",
        "    '''\n",
        "    Inverse function of cvPose2BulletView, i.e. returns extrinsic matrix from\n",
        "    PyBullet/OpenGL.\n",
        "    '''\n",
        "    Tc = np.array([[1,   0,    0,  0],\n",
        "                   [0,  -1,    0,  0],\n",
        "                   [0,   0,   -1,  0],\n",
        "                   [0,   0,    0,  1]]).reshape(4,4)\n",
        "    \n",
        "    T = np.linalg.inv(X.T)@Tc\n",
        "    \n",
        "    return T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY2sbo-8u1cL"
      },
      "source": [
        "## Create Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define classes\n",
        "\n",
        "Define observation and demo classes as per the conventions of [`YARR`](https://github.com/stepjam/YARR) and [`RLBench`](https://github.com/stepjam/RLBench). This is where we store observations/demos."
      ],
      "metadata": {
        "id": "kmwaFWO6jlcP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLWyCRp8u0up"
      },
      "outputs": [],
      "source": [
        "class Demo(object):\n",
        "\n",
        "    def __init__(self, observations, random_seed=None):\n",
        "        self._observations = observations\n",
        "        self.random_seed = random_seed\n",
        "        self.variation_number = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._observations)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._observations[i]\n",
        "\n",
        "    def restore_state(self):\n",
        "        np.random.set_state(self.random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Observation(object):\n",
        "    \"\"\"Storage for both visual and low-dimensional observations.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 left_shoulder_rgb: np.ndarray,\n",
        "                 left_shoulder_depth: np.ndarray,\n",
        "                 left_shoulder_mask: np.ndarray,\n",
        "                 left_shoulder_point_cloud: np.ndarray,\n",
        "                 right_shoulder_rgb: np.ndarray,\n",
        "                 right_shoulder_depth: np.ndarray,\n",
        "                 right_shoulder_mask: np.ndarray,\n",
        "                 right_shoulder_point_cloud: np.ndarray,\n",
        "                 overhead_rgb: np.ndarray,\n",
        "                 overhead_depth: np.ndarray,\n",
        "                 overhead_mask: np.ndarray,\n",
        "                 overhead_point_cloud: np.ndarray,\n",
        "                 wrist_rgb: np.ndarray,\n",
        "                 wrist_depth: np.ndarray,\n",
        "                 wrist_mask: np.ndarray,\n",
        "                 wrist_point_cloud: np.ndarray,\n",
        "                 front_rgb: np.ndarray,\n",
        "                 front_depth: np.ndarray,\n",
        "                 front_mask: np.ndarray,\n",
        "                 front_point_cloud: np.ndarray,\n",
        "                 joint_velocities: np.ndarray,\n",
        "                 joint_positions: np.ndarray,\n",
        "                 joint_forces: np.ndarray,\n",
        "                 gripper_open: float,\n",
        "                 gripper_pose: np.ndarray,\n",
        "                 gripper_matrix: np.ndarray,\n",
        "                 gripper_joint_positions: np.ndarray,\n",
        "                 gripper_touch_forces: np.ndarray,\n",
        "                 task_low_dim_state: np.ndarray,\n",
        "                 ignore_collisions: np.ndarray,\n",
        "                 misc: dict):\n",
        "        self.left_shoulder_rgb = left_shoulder_rgb\n",
        "        self.left_shoulder_depth = left_shoulder_depth\n",
        "        self.left_shoulder_mask = left_shoulder_mask\n",
        "        self.left_shoulder_point_cloud = left_shoulder_point_cloud\n",
        "        self.right_shoulder_rgb = right_shoulder_rgb\n",
        "        self.right_shoulder_depth = right_shoulder_depth\n",
        "        self.right_shoulder_mask = right_shoulder_mask\n",
        "        self.right_shoulder_point_cloud = right_shoulder_point_cloud\n",
        "        self.overhead_rgb = overhead_rgb\n",
        "        self.overhead_depth = overhead_depth\n",
        "        self.overhead_mask = overhead_mask\n",
        "        self.overhead_point_cloud = overhead_point_cloud\n",
        "        self.wrist_rgb = wrist_rgb\n",
        "        self.wrist_depth = wrist_depth\n",
        "        self.wrist_mask = wrist_mask\n",
        "        self.wrist_point_cloud = wrist_point_cloud\n",
        "        self.front_rgb = front_rgb\n",
        "        self.front_depth = front_depth\n",
        "        self.front_mask = front_mask\n",
        "        self.front_point_cloud = front_point_cloud\n",
        "        self.joint_velocities = joint_velocities\n",
        "        self.joint_positions = joint_positions\n",
        "        self.joint_forces = joint_forces\n",
        "        self.gripper_open = gripper_open\n",
        "        self.gripper_pose = gripper_pose\n",
        "        self.gripper_matrix = gripper_matrix\n",
        "        self.gripper_joint_positions = gripper_joint_positions\n",
        "        self.gripper_touch_forces = gripper_touch_forces\n",
        "        self.task_low_dim_state = task_low_dim_state\n",
        "        self.ignore_collisions = ignore_collisions\n",
        "        self.misc = misc"
      ],
      "metadata": {
        "id": "cdkE-5xbjZPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Type\n",
        "\n",
        "class ObservationElement(object):\n",
        "\n",
        "    def __init__(self, name: str, shape: tuple, type: Type[np.dtype]):\n",
        "        self.name = name\n",
        "        self.shape = shape\n",
        "        self.type = type"
      ],
      "metadata": {
        "id": "s_ytYtm0jf1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2rsOP6h1cRq"
      },
      "source": [
        "### Get Obs:\n",
        "\n",
        "We now define a function that saves the observation from panda_gym/PyBullet in accordance with [rlbench](https://github.com/stepjam/RLBench), [PerAct](https://github.com/peract/peract) and specifically the PyBulletPerAct colab file.\n",
        "\n",
        "Not everything implemented here is actually used in PyBulletPerAct but it is in accordance with the RLBench conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6ez0PDD0jzU"
      },
      "outputs": [],
      "source": [
        "def _normalize_fn(pc,far,near):\n",
        "  '''\n",
        "  helper \"normalization\" function for depth\n",
        "  '''\n",
        "  return (pc-near)/(far-near)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZyCaKQc1d2E"
      },
      "outputs": [],
      "source": [
        "# env has to be initialized:\n",
        "\n",
        "def get_observation(env) -> Observation:\n",
        "    '''\n",
        "    Function that extracts from panda_gym/PyBullet the data \n",
        "    (for RLbench conventions) at a specific time-step. One needs to be careful\n",
        "    to change conventions from PyBullet to RLBench.\n",
        "    '''\n",
        "\n",
        "    _sim_pc = env.sim.physics_client\n",
        "    _sim = env.sim\n",
        "    panda_id = _sim._bodies_idx['panda']\n",
        "\n",
        "    pos = np.array([_sim_pc.getJointState(panda_id,i)[0] \\\n",
        "            for i in range(7)])\n",
        "    vels = np.array([_sim_pc.getJointState(panda_id,i)[1] \\\n",
        "            for i in range(7)])\n",
        "    joint_forces = np.array([_sim_pc.getJointState(panda_id,i)[3] \\\n",
        "                    for i in range(7)])\n",
        "    \n",
        "    ee_forces_flat = np.array(_sim_pc.getJointState(panda_id,6)[2])\n",
        "\n",
        "    rgb, depth, mask, cam_misc = env.render(mode = 'rgb_array')\n",
        "\n",
        "    left_shoulder_rgb = rgb['left_shoulder']\n",
        "    # adjust depth conventions:\n",
        "    far = cam_misc['left_shoulder']['far']\n",
        "    near = cam_misc['left_shoulder']['near']\n",
        "    left_shoulder_depth = (far*near)/(far-(far-near)*depth['left_shoulder'])\n",
        "    # scale to be in [0,1]:\n",
        "    left_shoulder_depth = _normalize_fn(left_shoulder_depth,far,near)\n",
        "    # (Mask is not necessary for PyBulletPerAct)\n",
        "    left_shoulder_mask = mask['left_shoulder']\n",
        "\n",
        "    right_shoulder_rgb = rgb['right_shoulder']\n",
        "    # adjust depth conventions:\n",
        "    far = cam_misc['right_shoulder']['far']\n",
        "    near = cam_misc['right_shoulder']['near']\n",
        "    right_shoulder_depth = (far*near)/(far-(far-near)*depth['right_shoulder'])\n",
        "    # scale to be in [0,1]:\n",
        "    right_shoulder_depth = _normalize_fn(right_shoulder_depth,far,near)\n",
        "    # (Mask is not necessary for PyBulletPerAct)\n",
        "    right_shoulder_mask = mask['right_shoulder']\n",
        "\n",
        "    # if you want more camera angles follow the above formate\n",
        "    # overhead_rgb = ...\n",
        "    # overhead_depth = ...\n",
        "    # overhead_mask = ...\n",
        "\n",
        "    wrist_rgb = rgb['wrist']\n",
        "    # adjust depth conventions:\n",
        "    far = cam_misc['wrist']['far']\n",
        "    near = cam_misc['wrist']['near']\n",
        "    wrist_depth = (far*near)/(far-(far-near)*depth['wrist'])\n",
        "    # scale to be in [0,1]:\n",
        "    wrist_depth = _normalize_fn(wrist_depth,far,near)\n",
        "    # (Mask is not necessary for PyBulletPerAct)\n",
        "    wrist_mask = mask['wrist']\n",
        "    \n",
        "    front_rgb = rgb['front']\n",
        "    # adjust depth conventions:\n",
        "    far = cam_misc['front']['far']\n",
        "    near = cam_misc['front']['near']\n",
        "    front_depth = (far*near)/(far-(far-near)*depth['front'])\n",
        "    # scale to be in [0,1]:\n",
        "    front_depth = _normalize_fn(front_depth,far,near)\n",
        "    # (Mask is not necessary for PyBulletPerAct)\n",
        "    front_mask = mask['front']\n",
        "\n",
        "    gripper_state = _sim_pc.getLinkState(0,11)\n",
        "    gripper_pose = np.concatenate(gripper_state[:2])\n",
        "\n",
        "    gripper_rot_mat = np.array(_sim_pc.getMatrixFromQuaternion(gripper_state[1])).reshape(-1,3)\n",
        "    gripper_matrix = np.row_stack((np.column_stack((gripper_rot_mat,gripper_pose[:3])),np.array([0,0,0,1])))\n",
        "\n",
        "    gripper_joint_pos = np.array([_sim_pc.getJointState(0, i)[0] for i in range(9,11)])\n",
        "\n",
        "    glass_id = _sim._bodies_idx['object']\n",
        "    cp9_glass = len(_sim_pc.getContactPoints(panda_id,glass_id, linkIndexA = 9))\n",
        "    cp10_glass = len(_sim_pc.getContactPoints(panda_id,glass_id, linkIndexA = 10))\n",
        "    ignore_collisions = np.array(1 if (cp9_glass>0 and cp10_glass>0) else 0)\n",
        "\n",
        "    gripper_op = (1 if env.robot.get_fingers_width() > 0.09 else 0)\n",
        "    cp9 = len(_sim_pc.getContactPoints(bodyA = panda_id, linkIndexA = 9))\n",
        "    cp10 = len(_sim_pc.getContactPoints(bodyA = panda_id, linkIndexA = 9))\n",
        "    gripper_with_obj = (1 if (cp9==0 and cp10==0) else 0)\n",
        "\n",
        "    gripper_open = (1.0 if (gripper_op==1 and gripper_with_obj==1) else 0)\n",
        "\n",
        "    obs = Observation(\n",
        "        left_shoulder_rgb=left_shoulder_rgb,\n",
        "        left_shoulder_depth=left_shoulder_depth,\n",
        "        left_shoulder_point_cloud = None,\n",
        "        right_shoulder_rgb=right_shoulder_rgb,\n",
        "        right_shoulder_depth=right_shoulder_depth,\n",
        "        right_shoulder_point_cloud= None,\n",
        "        overhead_rgb = None, #overhead_rgb,\n",
        "        overhead_depth = None, #overhead_depth,\n",
        "        overhead_point_cloud = None, #overhead_pcd,\n",
        "        wrist_rgb=wrist_rgb,\n",
        "        wrist_depth=wrist_depth,\n",
        "        wrist_point_cloud = None,\n",
        "        front_rgb=front_rgb,\n",
        "        front_depth=front_depth,\n",
        "        front_point_cloud=None,\n",
        "        left_shoulder_mask=left_shoulder_mask,\n",
        "        right_shoulder_mask=right_shoulder_mask,\n",
        "        overhead_mask= None, #overhead_mask,\n",
        "        wrist_mask = wrist_mask,\n",
        "        front_mask = front_mask,\n",
        "        joint_velocities = vels,\n",
        "        joint_positions = pos,\n",
        "        joint_forces = joint_forces,\n",
        "        gripper_open = gripper_open, \n",
        "        gripper_pose = gripper_pose,\n",
        "        gripper_matrix = gripper_matrix,\n",
        "        gripper_touch_forces = ee_forces_flat,\n",
        "        gripper_joint_positions = gripper_joint_pos,\n",
        "        task_low_dim_state = get_low_dim_state(env),\n",
        "        ignore_collisions = ignore_collisions, # get correct object\n",
        "        misc=_get_misc(cam_misc))\n",
        "    return obs\n",
        "\n",
        "\n",
        "def get_low_dim_state(env) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Gets the pose and various other properties of objects in the task.\n",
        "    :return: 1D array of low-dimensional task state.\n",
        "\n",
        "    This needs some modifications if one wants to use this notebook\n",
        "    for tasks beyond PerAct.\n",
        "    \"\"\"\n",
        "\n",
        "    # Corner cases:\n",
        "    # (1) Object has been deleted.\n",
        "    # (2) Object has been grasped (and is now child of gripper).\n",
        "\n",
        "    state = []\n",
        "    '''\n",
        "    for obj, objtype in self._initial_objs_in_scene:\n",
        "        if not obj.still_exists():\n",
        "            # It has been deleted\n",
        "            empty_len = 7\n",
        "            if objtype == ObjectType.JOINT:\n",
        "                empty_len += 1\n",
        "            elif objtype == ObjectType.FORCE_SENSOR:\n",
        "                empty_len += 6\n",
        "            state.extend(np.zeros((empty_len,)).tolist())\n",
        "        else:\n",
        "            state.extend(np.array(obj.get_pose()))\n",
        "            if obj.get_type() == ObjectType.JOINT:\n",
        "                state.extend([Joint(obj.get_handle()).get_joint_position()])\n",
        "            elif obj.get_type() == ObjectType.FORCE_SENSOR:\n",
        "                forces, torques = ForceSensor(obj.get_handle()).read()\n",
        "                state.extend(forces + torques)\n",
        "    '''\n",
        "    # might include panda also, not sure -- don't think it matters.\n",
        "    for obj in env.sim._bodies_idx.keys():\n",
        "      if obj=='panda':\n",
        "        continue\n",
        "      else:\n",
        "        state.extend(np.array(get_pose(env,obj)))\n",
        "\n",
        "    return np.array(state).flatten()\n",
        "\n",
        "def get_pose(env,body) -> np.ndarray:\n",
        "    '''\n",
        "    Retrieves the position and quaternion of an object\n",
        "    :body: body of which we want the pose.\n",
        "    :return: An array containing the (X,Y,Z,Qx,Qy,Qz,Qw) pose of\n",
        "        the object.\n",
        "    '''\n",
        "    position,quaternion = env.sim.physics_client.getBasePositionAndOrientation(env.sim._bodies_idx[body])\n",
        "    return np.r_[position, quaternion]\n",
        "\n",
        "\n",
        "def _get_misc(misc_cam):\n",
        "    '''\n",
        "    :misc_cam: return function from render (a dictionary)\n",
        "    :returns: reformatted misc data (intr/extr/near/far) as a dictionary\n",
        "    '''\n",
        "\n",
        "    def _get_cam_data(misc_cam,name):\n",
        "        mc = misc_cam[name]\n",
        "        w,h = mc['width'], mc['height']\n",
        "        extrinsic = BullettocvPose(mc['view'])\n",
        "        intrinsic = BulletP2cvK(mc['proj'], w, h)[0]\n",
        "        \n",
        "        d = {\n",
        "            '%s_camera_extrinsics' % name: extrinsic,\n",
        "            '%s_camera_intrinsics' % name: intrinsic,\n",
        "            '%s_camera_near' % name: mc['near'],\n",
        "            '%s_camera_far' % name: mc['far'],\n",
        "        }\n",
        "        return d\n",
        "    misc = _get_cam_data(misc_cam, 'left_shoulder')\n",
        "    misc.update(_get_cam_data(misc_cam, 'right_shoulder'))\n",
        "    #misc.update(_get_cam_data(self._cam_overhead, 'overhead_camera'))\n",
        "    misc.update(_get_cam_data(misc_cam, 'front'))\n",
        "    misc.update(_get_cam_data(misc_cam, 'wrist'))\n",
        "    return misc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EXvzX3Pu440"
      },
      "source": [
        "## Save Demo\n",
        "\n",
        "We now implement basic functions that saves the demo in a way that feeds straightforwardly into the PyBulletPerAct colab file.\n",
        "\n",
        "**Rmk:** We do not implement variation number and variation description here. It is easy to modify the below function and add this if you want to save text prompts with your demos. I.e. simply save a pickle file as VARIATION_DESCRIPTIONS_PKL and uncomment the relevant lines in the PyBulletPerAct \"fill_replay\" function. I suggest you do this right away to simplify the pipeline..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO_eZ-d6UxUd"
      },
      "outputs": [],
      "source": [
        "IMAGE_FORMAT = '%d.png'\n",
        "\n",
        "LEFT_SHOULDER_RGB_FOLDER = 'left_shoulder_rgb'\n",
        "LEFT_SHOULDER_DEPTH_FOLDER = 'left_shoulder_depth'\n",
        "LEFT_SHOULDER_MASK_FOLDER = 'left_shoulder_mask'\n",
        "RIGHT_SHOULDER_RGB_FOLDER = 'right_shoulder_rgb'\n",
        "RIGHT_SHOULDER_DEPTH_FOLDER = 'right_shoulder_depth'\n",
        "RIGHT_SHOULDER_MASK_FOLDER = 'right_shoulder_mask'\n",
        "OVERHEAD_RGB_FOLDER = 'overhead_rgb'\n",
        "OVERHEAD_DEPTH_FOLDER = 'overhead_depth'\n",
        "OVERHEAD_MASK_FOLDER = 'overhead_mask'\n",
        "WRIST_RGB_FOLDER = 'wrist_rgb'\n",
        "WRIST_DEPTH_FOLDER = 'wrist_depth'\n",
        "WRIST_MASK_FOLDER = 'wrist_mask'\n",
        "FRONT_RGB_FOLDER = 'front_rgb'\n",
        "FRONT_DEPTH_FOLDER = 'front_depth'\n",
        "FRONT_MASK_FOLDER = 'front_mask'\n",
        "EPISODES_FOLDER = 'episodes'\n",
        "EPISODE_FOLDER = 'episode%d'\n",
        "VARIATIONS_FOLDER = 'variation%d'\n",
        "\n",
        "LOW_DIM_PICKLE = 'low_dim_obs.pkl'\n",
        "VARIATION_DESCRIPTIONS = 'variation_descriptions.pkl'\n",
        "\n",
        "DEPTH_SCALE = 2**24 - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIiHgn3DKekG"
      },
      "outputs": [],
      "source": [
        "def save_demo(demo, example_path):\n",
        "    '''\n",
        "    Function that saves a demo (demo) (generated by using save_observation as well as\n",
        "    observation and demo classes defined above) to a certain example_path \n",
        "    (e.g. on your google drive).\n",
        "\n",
        "    The way this is saved will allow for feeding the data into the PyBulletPerAct\n",
        "    colab file.\n",
        "    '''\n",
        "    # Save image data first, and then None the image data, and pickle\n",
        "    left_shoulder_rgb_path = os.path.join(\n",
        "        example_path, LEFT_SHOULDER_RGB_FOLDER)\n",
        "    left_shoulder_depth_path = os.path.join(\n",
        "        example_path, LEFT_SHOULDER_DEPTH_FOLDER)\n",
        "    left_shoulder_mask_path = os.path.join(\n",
        "        example_path, LEFT_SHOULDER_MASK_FOLDER)\n",
        "    right_shoulder_rgb_path = os.path.join(\n",
        "        example_path, RIGHT_SHOULDER_RGB_FOLDER)\n",
        "    right_shoulder_depth_path = os.path.join(\n",
        "        example_path, RIGHT_SHOULDER_DEPTH_FOLDER)\n",
        "    right_shoulder_mask_path = os.path.join(\n",
        "        example_path, RIGHT_SHOULDER_MASK_FOLDER)\n",
        "    # you can add other camera angles such as overhead (this won't natively be\n",
        "    # used in PyBulletPerAct but can easily be implemented. The upshot is that \n",
        "    # the voxelization might improve):\n",
        "    # overhead_rgb_path = os.path.join(\n",
        "    #    example_path, OVERHEAD_RGB_FOLDER)\n",
        "    # overhead_depth_path = os.path.join(\n",
        "    #    example_path, OVERHEAD_DEPTH_FOLDER)\n",
        "    # overhead_mask_path = os.path.join(\n",
        "    #    example_path, OVERHEAD_MASK_FOLDER)\n",
        "    wrist_rgb_path = os.path.join(example_path, WRIST_RGB_FOLDER)\n",
        "    wrist_depth_path = os.path.join(example_path, WRIST_DEPTH_FOLDER)\n",
        "    wrist_mask_path = os.path.join(example_path, WRIST_MASK_FOLDER)\n",
        "    front_rgb_path = os.path.join(example_path, FRONT_RGB_FOLDER)\n",
        "    front_depth_path = os.path.join(example_path, FRONT_DEPTH_FOLDER)\n",
        "    front_mask_path = os.path.join(example_path, FRONT_MASK_FOLDER)\n",
        "\n",
        "    check_and_make(left_shoulder_rgb_path)\n",
        "    check_and_make(left_shoulder_depth_path)\n",
        "    check_and_make(left_shoulder_mask_path)\n",
        "    check_and_make(right_shoulder_rgb_path)\n",
        "    check_and_make(right_shoulder_depth_path)\n",
        "    check_and_make(right_shoulder_mask_path)\n",
        "    # check_and_make(overhead_rgb_path)\n",
        "    # check_and_make(overhead_depth_path)\n",
        "    # check_and_make(overhead_mask_path)\n",
        "    check_and_make(wrist_rgb_path)\n",
        "    check_and_make(wrist_depth_path)\n",
        "    check_and_make(wrist_mask_path)\n",
        "    check_and_make(front_rgb_path)\n",
        "    check_and_make(front_depth_path)\n",
        "    check_and_make(front_mask_path)\n",
        "\n",
        "    for i, obs in enumerate(demo):\n",
        "        left_shoulder_rgb = Image.fromarray(obs.left_shoulder_rgb)\n",
        "        left_shoulder_depth = float_array_to_rgb_image(\n",
        "            obs.left_shoulder_depth, scale_factor=DEPTH_SCALE)\n",
        "        left_shoulder_mask = Image.fromarray(\n",
        "            (obs.left_shoulder_mask * 255).astype(np.uint8))\n",
        "        right_shoulder_rgb = Image.fromarray(obs.right_shoulder_rgb)\n",
        "        right_shoulder_depth = float_array_to_rgb_image(\n",
        "            obs.right_shoulder_depth, scale_factor=DEPTH_SCALE)\n",
        "        right_shoulder_mask = Image.fromarray(\n",
        "            (obs.right_shoulder_mask * 255).astype(np.uint8))\n",
        "        # overhead_rgb = Image.fromarray(obs.overhead_rgb)\n",
        "        # overhead_depth = float_array_to_rgb_image(\n",
        "        #    obs.overhead_depth, scale_factor=DEPTH_SCALE)\n",
        "        # overhead_mask = Image.fromarray(\n",
        "        #    (obs.overhead_mask * 255).astype(np.uint8))\n",
        "        wrist_rgb = Image.fromarray(obs.wrist_rgb)\n",
        "        wrist_depth = float_array_to_rgb_image(\n",
        "            obs.wrist_depth, scale_factor=DEPTH_SCALE)\n",
        "        wrist_mask = Image.fromarray((obs.wrist_mask * 255).astype(np.uint8))\n",
        "        front_rgb = Image.fromarray(obs.front_rgb)\n",
        "        front_depth = float_array_to_rgb_image(\n",
        "            obs.front_depth, scale_factor=DEPTH_SCALE)\n",
        "        front_mask = Image.fromarray((obs.front_mask * 255).astype(np.uint8))\n",
        "\n",
        "        left_shoulder_rgb.save(\n",
        "            os.path.join(left_shoulder_rgb_path, IMAGE_FORMAT % i))\n",
        "        left_shoulder_depth.save(\n",
        "            os.path.join(left_shoulder_depth_path, IMAGE_FORMAT % i))\n",
        "        left_shoulder_mask.save(\n",
        "            os.path.join(left_shoulder_mask_path, IMAGE_FORMAT % i))\n",
        "        right_shoulder_rgb.save(\n",
        "            os.path.join(right_shoulder_rgb_path, IMAGE_FORMAT % i))\n",
        "        right_shoulder_depth.save(\n",
        "            os.path.join(right_shoulder_depth_path, IMAGE_FORMAT % i))\n",
        "        right_shoulder_mask.save(\n",
        "            os.path.join(right_shoulder_mask_path, IMAGE_FORMAT % i))\n",
        "        # overhead_rgb.save(\n",
        "        #    os.path.join(overhead_rgb_path, IMAGE_FORMAT % i))\n",
        "        # overhead_depth.save(\n",
        "        #    os.path.join(overhead_depth_path, IMAGE_FORMAT % i))\n",
        "        # overhead_mask.save(\n",
        "        #    os.path.join(overhead_mask_path, IMAGE_FORMAT % i))\n",
        "        wrist_rgb.save(os.path.join(wrist_rgb_path, IMAGE_FORMAT % i))\n",
        "        wrist_depth.save(os.path.join(wrist_depth_path, IMAGE_FORMAT % i))\n",
        "        wrist_mask.save(os.path.join(wrist_mask_path, IMAGE_FORMAT % i))\n",
        "        front_rgb.save(os.path.join(front_rgb_path, IMAGE_FORMAT % i))\n",
        "        front_depth.save(os.path.join(front_depth_path, IMAGE_FORMAT % i))\n",
        "        front_mask.save(os.path.join(front_mask_path, IMAGE_FORMAT % i))\n",
        "\n",
        "        # We save the images separately, so set these to None for pickling.\n",
        "        obs.left_shoulder_rgb = None\n",
        "        obs.left_shoulder_depth = None\n",
        "        obs.left_shoulder_point_cloud = None\n",
        "        obs.left_shoulder_mask = None\n",
        "        obs.right_shoulder_rgb = None\n",
        "        obs.right_shoulder_depth = None\n",
        "        obs.right_shoulder_point_cloud = None\n",
        "        obs.right_shoulder_mask = None\n",
        "        # obs.overhead_rgb = None\n",
        "        # obs.overhead_depth = None\n",
        "        # obs.overhead_point_cloud = None\n",
        "        # obs.overhead_mask = None\n",
        "        obs.wrist_rgb = None\n",
        "        obs.wrist_depth = None\n",
        "        obs.wrist_point_cloud = None\n",
        "        obs.wrist_mask = None\n",
        "        obs.front_rgb = None\n",
        "        obs.front_depth = None\n",
        "        obs.front_point_cloud = None\n",
        "        obs.front_mask = None\n",
        "\n",
        "    # Save the low-dimension data\n",
        "    with open(os.path.join(example_path, LOW_DIM_PICKLE), 'wb') as f:\n",
        "        pickle.dump(demo, f)\n",
        "\n",
        "    # If you want to save specific text prompts (needs some small modifications in \n",
        "    # PyBulletPerAct this is the place to add it as a pickle file)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODE_FOLDER = 'episode%d'\n",
        "\n",
        "CAMERA_FRONT = 'front'\n",
        "CAMERA_LS = 'left_shoulder'\n",
        "CAMERA_RS = 'right_shoulder'\n",
        "CAMERA_WRIST = 'wrist'\n",
        "CAMERAS = [CAMERA_FRONT, CAMERA_LS, CAMERA_RS, CAMERA_WRIST]\n",
        "\n",
        "IMAGE_RGB = 'rgb'\n",
        "IMAGE_DEPTH = 'depth'\n",
        "IMAGE_TYPES = [IMAGE_RGB, IMAGE_DEPTH]\n",
        "IMAGE_FORMAT  = '%d.png'\n",
        "LOW_DIM_PICKLE = 'low_dim_obs.pkl'\n",
        "\n",
        "def check_demo(data_path, index):\n",
        "  '''\n",
        "  Function that checks whether all image files for a demo have been stored succesfully.\n",
        "\n",
        "  It is worth doing this check in order to ensure PyBulletPerAct runs without issues\n",
        "  with the data generated here.\n",
        "\n",
        "  data_path = path where the demo is stored\n",
        "  indes = number of demo stored in this run.\n",
        "  '''\n",
        "\n",
        "  episode_path = os.path.join(data_path, EPISODE_FOLDER % index)\n",
        "  \n",
        "  # low dim pickle file\n",
        "  with open(os.path.join(episode_path, LOW_DIM_PICKLE), 'rb') as f:\n",
        "    obs = pickle.load(f)\n",
        "  \n",
        "  num_steps = len(obs)\n",
        "  for i in range(num_steps):\n",
        "    front_rgb = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_FRONT, IMAGE_RGB), IMAGE_FORMAT % i))\n",
        "    left_shoulder_rgb = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_LS, IMAGE_RGB), IMAGE_FORMAT % i))\n",
        "    right_shoulder_rgb = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_RS, IMAGE_RGB), IMAGE_FORMAT % i))\n",
        "    wrist_rgb = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_WRIST, IMAGE_RGB), IMAGE_FORMAT % i))\n",
        "\n",
        "    front_depth = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_FRONT, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "    left_shoulder_depth = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_LS, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "    right_shoulder_depth = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_RS, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "    wrist_depth = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_WRIST, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "\n",
        "    front_pc = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_FRONT, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "    left_shoulder_pc = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_LS, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "    right_shoulder_pc = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_RS, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "    wrist_pc = os.path.isfile(os.path.join(episode_path, '%s_%s' % (CAMERA_WRIST, IMAGE_DEPTH), IMAGE_FORMAT % i))\n",
        "\n",
        "    # mask not necessary for PyBulletPerAct but might be for other applications.\n",
        "    \n",
        "    tot = [front_rgb,left_shoulder_rgb,right_shoulder_rgb,wrist_rgb,\\\n",
        "           front_depth,left_shoulder_depth,right_shoulder_depth,wrist_depth,\\\n",
        "           front_pc,left_shoulder_pc,right_shoulder_pc,wrist_pc]\n",
        "\n",
        "    if not all(tot):\n",
        "      return False, i\n",
        "\n",
        "  return True, -1"
      ],
      "metadata": {
        "id": "zYssRaFtnKTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OfyHcM0TCV2"
      },
      "source": [
        "Let's do a quick check to see whether pointclouds look reasonable upon storage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBMrNRHZTHvI"
      },
      "outputs": [],
      "source": [
        "test_obs = get_observation(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3SldAGHS9qu"
      },
      "outputs": [],
      "source": [
        "float_array_to_rgb_image(test_obs.front_depth, scale_factor=DEPTH_SCALE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9dM467t527x"
      },
      "source": [
        "## Create Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some basic utils for panda control\n",
        "\n",
        "Here, we introduce several (naive) hard-coded motion-planning functions that help control the panda robot.\n",
        "\n",
        "In order to create more robust training data, it might be beneficial to implement certain more advanced motion-planners (e.g. that have capaticy to avoid collision, etc). See for instance [this approach](https://github.com/yijiangh/pybullet_planning/issues/7)."
      ],
      "metadata": {
        "id": "s21xjVfxkLp-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8-rcGAi55zi"
      },
      "outputs": [],
      "source": [
        "def distance(x,y):\n",
        "  '''\n",
        "  distance squared function between x and y in meters^2\n",
        "  '''\n",
        "  return (np.sum((x-y)**2))\n",
        "\n",
        "def approach_target(goal, from_side = True, ornt = np.array([1.0,0.0,0.0,0.0]), grip = 0, max_steps = 50, precision = 0.001):\n",
        "  '''\n",
        "  Controls panda robot to approach a target (goal) to a certain precision (in meters) \n",
        "  within a maximum number (max_steps) of steps and with gripper state given by grip.\n",
        "\n",
        "  If from_side = True, the robot's gripper orientation will remain the same as with our choice\n",
        "  of neutral angles otherwise setting from_side = False one can choose another orientation by \n",
        "  specifying ornt. The standard for ornt is pointing down.\n",
        "  \n",
        "  '''\n",
        "\n",
        "  tot_obs=[]\n",
        "  ee_position = env.robot.get_ee_position()\n",
        "  steps=0\n",
        "  \n",
        "  while np.sqrt(distance(goal,ee_position)) > precision and steps<max_steps:\n",
        "\n",
        "    multiplier = 5.0 if distance(goal,ee_position)>0.005 else 10.0\n",
        "    ee_displacement = multiplier * (goal - ee_position)\n",
        "    target_ee_position = ee_position + ee_displacement\n",
        "    target_ee_position[2] = np.max((0, target_ee_position[2]))\n",
        "\n",
        "    if from_side:\n",
        "      ornt = np.array([-0.07942096,  0.72870646, -0.00803236,  0.68015784])\n",
        "\n",
        "    target_arm_angles = env.robot.inverse_kinematics(link=11, \\\n",
        "                                                      position = target_ee_position, \\\n",
        "                                                      orientation = ornt\n",
        "                                                      )\n",
        "    target_arm_angles = target_arm_angles[:7]\n",
        "\n",
        "    action = target_arm_angles - np.array([env.robot.get_joint_angle(joint=i) for i in range(7)])\n",
        "    action = np.concatenate((action, [grip]))\n",
        "\n",
        "    current_obs = get_observation(env)\n",
        "    tot_obs.append(current_obs)\n",
        "\n",
        "    _, _, _, _ = env.step(action)\n",
        "    ee_position = env.robot.get_ee_position()\n",
        "    steps+=1\n",
        "  \n",
        "  current_obs = get_observation(env)\n",
        "  tot_obs.append(current_obs)\n",
        "  \n",
        "  print(steps)\n",
        "  return tot_obs\n",
        "\n",
        "def close_gripper():\n",
        "  '''\n",
        "  function that closes the gripper over 15 steps (feel free to adjust)\n",
        "  '''\n",
        "  tot_obs=[]\n",
        "\n",
        "  for _ in range(15):\n",
        "    action = np.concatenate(([0,0,0,0,0,0,0], [-1]))\n",
        "    current_obs = get_observation(env)\n",
        "    tot_obs.append(current_obs)\n",
        "\n",
        "    _,_,_,_ = env.step(action)\n",
        "  \n",
        "  current_obs = get_observation(env)\n",
        "  tot_obs.append(current_obs)\n",
        "\n",
        "  return tot_obs\n",
        "\n",
        "def pour(angle_shift = 0, right = True):\n",
        "  '''\n",
        "  Function that \"pours\" until a certain angle (angle_shift) of the gripper \n",
        "  from the position before using the pour function has been reached. Effectively,\n",
        "  this is just a function that turns the 6th joint of the robot up until it has \n",
        "  reached a certain angle_shift.\n",
        "\n",
        "  Ideally, one implements a way to pour from the left and from the right, but the way\n",
        "  the neutral angles of the panda robot are chosen, pouring from right does not seem\n",
        "  to work reliably.\n",
        "  '''\n",
        "\n",
        "  tot_obs=[]\n",
        "  og_angle = env.sim.physics_client.getJointState(0,6)[0]#starting angle is at math.pi/4 \n",
        "  angle = env.sim.physics_client.getJointState(0,6)[0]\n",
        "  steps = 0\n",
        "  action = (np.concatenate(([0,0,0,0,0,0,-1], [-1])) if right else np.concatenate(([0,0,0,0,0,0,1], [-1])))\n",
        "  if not right: # only right=false works reliably\n",
        "    while og_angle+angle_shift > angle: \n",
        "      current_obs = get_observation(env)\n",
        "      tot_obs.append(current_obs)\n",
        "\n",
        "      _, _, _, _ = env.step(action)\n",
        "      angle = env.sim.physics_client.getJointState(0,6)[0]\n",
        "      steps +=1\n",
        "  else:\n",
        "    while og_angle-angle_shift < angle:\n",
        "      current_obs = get_observation(env)\n",
        "      tot_obs.append(current_obs)\n",
        "\n",
        "      _, _, _, _ = env.step(action)\n",
        "      angle = env.sim.physics_client.getJointState(0,6)[0]\n",
        "      steps +=1\n",
        "    \n",
        "    #last obs\n",
        "    current_obs = get_observation(env)\n",
        "    tot_obs.append(current_obs)\n",
        "\n",
        "    _, _, _, _ = env.step(action)\n",
        "    angle = env.sim.physics_client.getJointState(0,6)[0]\n",
        "    steps +=1\n",
        "  \n",
        "  current_obs = get_observation(env)\n",
        "  tot_obs.append(current_obs)\n",
        "  print(steps)\n",
        "  return tot_obs\n",
        "\n",
        "def count_nr_in_tg(env):\n",
        "  '''\n",
        "  Simple function that counts the number of spheres that have landed in the \n",
        "  red cup, remained in the blue cup and landed on the table. This function should \n",
        "  only be used (reliably) once nothing is moving anymore.\n",
        "  '''\n",
        "\n",
        "\n",
        "  obj=env.sim.get_base_position('object')\n",
        "  tg=env.sim.get_base_position('target')\n",
        "\n",
        "  nr_in_tg=0\n",
        "  nr_in_obj = 0\n",
        "  nr_in_tbl = 0\n",
        "  dist=[]\n",
        "\n",
        "  for i in range(env.task.sph):\n",
        "      for j in range(env.task.sph):\n",
        "          for k in range(env.task.sph):\n",
        "              pos = env.sim.get_base_position('s'+str(i)+str(j)+str(k))\n",
        "              \n",
        "              dist.append((np.sqrt(distance(pos,tg)),np.sqrt(distance(pos,obj))))\n",
        "              #if np.sqrt(distance(pos,tg)) > 0.02 and np.sqrt(distance(pos,obj)) > 0.02:\n",
        "              if pos[2]>0.01:\n",
        "                  nr_in_obj+=1\n",
        "              elif np.sqrt(distance(pos[:2],tg[:2])) < 0.055:\n",
        "                  nr_in_tg+=1\n",
        "              else:\n",
        "                  nr_in_tbl+=1\n",
        "\n",
        "  return nr_in_tg, nr_in_obj, nr_in_tbl, dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMSiCyP4NaBo"
      },
      "outputs": [],
      "source": [
        "#env.close()\n",
        "env = gym.make('PandaPickAndPlace-v2',control_type='joints')\n",
        "obs = env.reset()\n",
        "\n",
        "\n",
        "# Use these neutral joint values to best pick up the cup from the side:\n",
        "env.robot.neutral_joint_values = np.array([0.00, 0.41, 0.00, -1.85, 3.14, 2.52, 0.79, 3.00, 3.00])\n",
        "\n",
        "# Turn on torque sensors:\n",
        "[(env.sim.physics_client.enableJointForceTorqueSensor(0,i)) for i in range(11)];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "XST1xS12Nh-Z",
        "outputId": "cd14b25c-c29c-4d9b-f49f-36dd8e9ffb5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 128x128 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "rgb, depth, mask, misc = env.render(mode = 'rgb_array')\n",
        "display.HTML(display_anim([rgb['front']]));"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic example for \"pick, place and pour\"\n",
        "\n",
        "We can now now explicitly generate a dataset. The below code is a (relatively naive) implementation of picking up the blue cup and putting it above (to the left) of the red one. In principle, one can append a pouring-action (e.g. by uncommenting some of the steps below), though it seems more efficient to train the model on pouring independently and then using the text-prompt in PerAct to combine the two.\n",
        "\n",
        "In practice, I would propose to generate a dataset for the prompt \"pick up the blue cup and move it close to the red one\" and one for \"pour x balls from the red cup into the blue one\". Then train PerAct on this data and prompts and eventually test it on prompts such as \"pick up the blue cup, move it close to the red one and pour x balls into it\".\n",
        "\n",
        "In any case, the interesting (and largely unexplored) part here is whether one can train PerAct to pour accurately a certain number of balls.\n",
        "\n",
        "The approach in the below code is very naive by giving various points \"waypoints\" to approach and pick up the blue cup. I think it's worth investing some time to improve this and in particular add some diversity in the path, which will improve training."
      ],
      "metadata": {
        "id": "Czhqn7n4kZH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "i=0\n",
        "while i<20:\n",
        "    obs = env.reset()\n",
        "    og_ee_pos = env.robot.get_ee_position()\n",
        "    ee_position = env.robot.get_ee_position()\n",
        "\n",
        "    obs = []\n",
        "\n",
        "    print('-----',i,'-----')\n",
        "    goal = env.sim.get_base_position(\"object\") - np.array([0.08,0.0,0])\n",
        "    goal = np.concatenate((goal[:2],[og_ee_pos[2]]))\n",
        "\n",
        "    obs+=(approach_target(goal, from_side = True, grip = 1, precision = 0.04))\n",
        "\n",
        "    print('-----')\n",
        "    goal = np.concatenate((goal[:2],[env.sim.get_base_position(\"object\")[2]]))\n",
        "    obs+=(approach_target(goal, from_side = True, grip = 1, precision = 0.04))\n",
        "\n",
        "    print('-----')\n",
        "    goal = env.sim.get_base_position(\"object\") + np.array([0.04,0,0.0175])\n",
        "    obs+=(approach_target(goal, from_side = True, grip = 0, precision = 0.01))\n",
        "\n",
        "    print('-----')\n",
        "    goal = goal+np.array([0,0,0.25])\n",
        "    obs+=(approach_target(goal, from_side = True, grip = -1, precision = 0.04))\n",
        "\n",
        "    #print('-----')\n",
        "    #goal = env.sim.get_base_position(\"target\") \n",
        "    #obs+=(approach_target(goal, from_side = True, grip = -1))\n",
        "\n",
        "    print('-----')\n",
        "    goal = env.sim.get_base_position(\"target\") + np.array([0,0,0.16])\n",
        "    #if goal[1]>env.sim.get_base_position(\"object\")[1]:\n",
        "    #  offset = np.array([0,-0.02,0])\n",
        "    #  right = False \n",
        "    #else:\n",
        "    #  offset = np.array([0,0.02,0])\n",
        "    #  right = True \n",
        "\n",
        "    right = False\n",
        "    offset = np.array([0,0.12,0])\n",
        "\n",
        "    goal+=offset\n",
        "    obs+=(approach_target(goal, from_side = True, grip = -1,precision = 0.001))\n",
        "\n",
        "    print('-----')\n",
        "\n",
        "    # if you want to include the pouring action:\n",
        "    #obs+=pour(angle_shift=math.pi/2 + math.pi/2*(0.3),right = right)\n",
        "    #print('-----')\n",
        "    #obs+=pour(angle_shift=math.pi/2 + math.pi/8*(0.3),right = not right)\n",
        "\n",
        "    #count nr in red, blue or spilled:\n",
        "    #_,_,_,dd=count_nr_in_tg(env)\n",
        "    #print(count_nr_in_tg(env)[:3])\n",
        "\n",
        "    final_dist_= np.sqrt(distance(env.sim.get_base_position(\"target\"),env.sim.get_base_position(\"object\")))\n",
        "    print(f'Final distance between blue and red cup: {final_dist_:.3f}m')\n",
        "    print('visual check final frame:')\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(obs[-1].front_rgb, interpolation='nearest')\n",
        "    display.display(fig)\n",
        "\n",
        "    panda_demo = Demo(obs)\n",
        "    EPISODE_FOLDER = 'episode%d'\n",
        "    index = i\n",
        "    panda_dataset_dir_eps = os.path.join(panda_dataset_dir, EPISODE_FOLDER % index)\n",
        "    if os.path.exists(panda_dataset_dir_eps):\n",
        "      # be careful to remove potential previously stored demos for the next index\n",
        "      !rm -rf $panda_dataset_dir_eps\n",
        "      \n",
        "    save_demo(panda_demo, panda_dataset_dir_eps)\n",
        "\n",
        "    check_save = check_demo(panda_dataset_dir, index)\n",
        "    if check_save:\n",
        "      i+=1\n"
      ],
      "metadata": {
        "id": "EmaZj_ntgOFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redo non-successful demos (upon checking the rendered final frames):"
      ],
      "metadata": {
        "id": "8SWTIGD6mvc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "redo_demos = [2,4,7,10,13,19]\n",
        "\n",
        "\n",
        "i=redo_demos.pop()\n",
        "while True:\n",
        "    obs = env.reset()\n",
        "    og_ee_pos = env.robot.get_ee_position()\n",
        "    ee_position = env.robot.get_ee_position()\n",
        "\n",
        "    obs = []\n",
        "\n",
        "    print('-----',i,'-----')\n",
        "    goal = env.sim.get_base_position(\"object\") - np.array([0.08,0.0,0])\n",
        "    goal = np.concatenate((goal[:2],[og_ee_pos[2]]))\n",
        "\n",
        "    obs+=(approach_target(goal, from_side = True, grip = 1, precision = 0.04))\n",
        "\n",
        "    print('-----')\n",
        "    goal = np.concatenate((goal[:2],[env.sim.get_base_position(\"object\")[2]]))\n",
        "    obs+=(approach_target(goal, from_side = True, grip = 1, precision = 0.04))\n",
        "\n",
        "    print('-----')\n",
        "    goal = env.sim.get_base_position(\"object\") + np.array([0.04,0,0.0175])\n",
        "    obs+=(approach_target(goal, from_side = True, grip = 0, precision = 0.01))\n",
        "\n",
        "    print('-----')\n",
        "    goal = goal+np.array([0,0,0.25])\n",
        "    obs+=(approach_target(goal, from_side = True, grip = -1, precision = 0.04))\n",
        "\n",
        "    #print('-----')\n",
        "    #goal = env.sim.get_base_position(\"target\") \n",
        "    #obs+=(approach_target(goal, from_side = True, grip = -1))\n",
        "\n",
        "    print('-----')\n",
        "    goal = env.sim.get_base_position(\"target\") + np.array([0,0,0.16])\n",
        "    #if goal[1]>env.sim.get_base_position(\"object\")[1]:\n",
        "    #  offset = np.array([0,-0.02,0])\n",
        "    #  right = False \n",
        "    #else:\n",
        "    #  offset = np.array([0,0.02,0])\n",
        "    #  right = True \n",
        "\n",
        "    right = False\n",
        "    offset = np.array([0,0.12,0])\n",
        "\n",
        "    goal+=offset\n",
        "    obs+=(approach_target(goal, from_side = True, grip = -1,precision = 0.001))\n",
        "\n",
        "    print('-----')\n",
        "\n",
        "    # if you want to include the pouring action:\n",
        "    #obs+=pour(angle_shift=math.pi/2 + math.pi/2*(0.3),right = right)\n",
        "    #print('-----')\n",
        "    #obs+=pour(angle_shift=math.pi/2 + math.pi/8*(0.3),right = not right)\n",
        "\n",
        "    #count nr in red, blue or spilled:\n",
        "    #_,_,_,dd=count_nr_in_tg(env)\n",
        "    #print(count_nr_in_tg(env)[:3])\n",
        "\n",
        "    final_dist_= np.sqrt(distance(env.sim.get_base_position(\"target\"),env.sim.get_base_position(\"object\")))\n",
        "    print(f'Final distance between blue and red cup: {final_dist_:.3f}m')\n",
        "    print('visual check final frame:')\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(obs[-1].front_rgb, interpolation='nearest')\n",
        "    display.display(fig)\n",
        "\n",
        "    panda_demo = Demo(obs)\n",
        "    EPISODE_FOLDER = 'episode%d'\n",
        "    index = i\n",
        "    panda_dataset_dir_eps = os.path.join(panda_dataset_dir, EPISODE_FOLDER % index)\n",
        "    if os.path.exists(panda_dataset_dir_eps):\n",
        "      !rm -rf $panda_dataset_dir_eps\n",
        "\n",
        "    save_demo(panda_demo, panda_dataset_dir_eps)\n",
        "\n",
        "    check_save = check_demo(panda_dataset_dir, index)\n",
        "    if check_save:\n",
        "      if redo_demos:\n",
        "        i = redo_demos.pop()\n",
        "      else:\n",
        "        break"
      ],
      "metadata": {
        "id": "WNb_nikh6bGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data_pnp.zip PyBulletPerAct/data"
      ],
      "metadata": {
        "id": "dU2tD9FGpdSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('data_pnp.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AIAhSc02ptG2",
        "outputId": "d76f9b04-2594-488d-ed63-f1261bcbbd9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a246c828-d3c3-41c8-ad7b-ad2e7d2265f4\", \"data_pnp.zip\", 174281622)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}