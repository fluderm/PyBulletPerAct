{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fluderm/PyBulletPerAct/blob/main/PyBulletPerAct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNovH9jQPdTJ"
      },
      "source": [
        "# **PyBulletPerAct:** \n",
        "\n",
        "The following notebook implements the PerAct training pipeline. It requires saved data (you can download examples of pick and place from my google drive -- see below). In order to generate your own data, run the [Create_and_save_demo](https://github.com/fellowship/Robotics-Simulator-Imitation-Learning/blob/main/Create_and_save_demo.ipynb) notebook.\n",
        "\n",
        "A lot of the following is code adapted from:\n",
        "\n",
        "*   [Perceiver-Actor (PerAct)](https://github.com/peract/peract) -- see also the relevant [paper](https://peract.github.io/) and the [colab](https://colab.research.google.com/drive/1wpaosDS94S0rmtGmdnP0J1TjS7mEM14V?usp=sharing)\n",
        "*   [`ARM`](https://github.com/stepjam/ARM) \n",
        "*   [`YARR`](https://github.com/stepjam/YARR)\n",
        "*   [`PyRep`](https://github.com/stepjam/PyRep)\n",
        "*   [`RLBench`](https://github.com/stepjam/RLBench) see also the [paper](https://stepjam.github.io/) \n",
        "*   [PerceiverIO](https://arxiv.org/abs/2107.14795) from [`perceiver-pytorch`](https://github.com/lucidrains/perceiver-pytorch) \n",
        "*   [this LAMB implementation](https://github.com/cybertronai/pytorch-lamb).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY8iX-eLYepQ"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jM6GveIUIlW"
      },
      "outputs": [],
      "source": [
        "!pip install scipy==1.4.1 ftfy regex tqdm torch==1.7.1 git+https://github.com/openai/CLIP.git einops pyrender==0.1.45 trimesh==3.9.34 pycollada==0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kb4kpb-wtLV"
      },
      "outputs": [],
      "source": [
        "%pip install setuptools==65.5.0 # Need to add this for panda-gym==2.0.0 to work\n",
        "!apt install -y python-opengl ffmpeg > /dev/null 2>&1\n",
        "%pip install pyvirtualdisplay\n",
        "\n",
        "#stable_baselines3 sb3_contrib panda-gym==2.0.0 \n",
        "!pip install pyvirtualdisplay \n",
        "!apt install xvfb python-opengl ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGIz2RohYkJ9"
      },
      "source": [
        "### Clone Repos and Setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJU62Vd5kJ99"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/peract/peract_colab.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avT7JfBKc3Ya"
      },
      "source": [
        "If you fork-off this repo, you might want to pull the latest changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cERy2oElEuT"
      },
      "outputs": [],
      "source": [
        "!cd peract_colab && git pull origin master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fluderm/PyBulletPerAct.git\n",
        "%pip install -e PyBulletPerAct\n",
        "\n",
        "# copy glass files to current directory:\n",
        "\n",
        "!cp PyBulletPerAct/panda_gym/plastic_coffee_cup.mtl \\\n",
        " PyBulletPerAct/panda_gym/plastic_coffee_cup_vhacd.obj \\\n",
        " PyBulletPerAct/panda_gym/plastic_coffee_cup.obj ."
      ],
      "metadata": {
        "id": "izII_sFpFWUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download data\n",
        "\n",
        "Let us download some data"
      ],
      "metadata": {
        "id": "tzqYvcUXJEi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "Fetch pick_and_place data generated in create_and_save and saved on my google\n",
        "drive as a zip file\n",
        "'''\n",
        "\n",
        "!gdown 1YQOpEDCtGUgDgE6FiK-hpd2ON4y9O39S"
      ],
      "metadata": {
        "id": "IqqcZwJh6r2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip data to current directory\n",
        "\n",
        "!unzip data_pnp.zip -d Robotics-Simulator-Imitation-Learning/PyBulletPerAct-main/pnp"
      ],
      "metadata": {
        "id": "nDEGwJSh69j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12mooJuWggGm"
      },
      "source": [
        "# Panda Gym\n",
        "\n",
        "Importing panda gym is technically not necessary, but it is used to check a variety of things below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "from PIL import Image\n",
        "#from absl import app\n",
        "#from absl import flags\n",
        "\n",
        "sys.path.append('PyBulletPerAct')\n",
        "sys.path.append('peract_colab')"
      ],
      "metadata": {
        "id": "-a0TwwvwFuTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6T-n2Cpmxo6"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import panda_gym\n",
        "import pprint\n",
        "import numpy as np\n",
        "import pybullet as p\n",
        "import math\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "from matplotlib import pyplot as plt, animation\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "'''\n",
        "The following are some basic functions that help render the virtual PyBullet\n",
        "environment:\n",
        "'''\n",
        "\n",
        "def create_anim(frames, dpi, fps):\n",
        "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    def setup():\n",
        "        plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n",
        "    return anim\n",
        "\n",
        "def display_anim(frames, dpi=72, fps=50):\n",
        "    anim = create_anim(frames, dpi, fps)\n",
        "    return anim.to_jshtml()\n",
        "\n",
        "def save_anim(frames, filename, dpi=72, fps=50):\n",
        "    anim = create_anim(frames, dpi, fps)\n",
        "    anim.save(filename)\n",
        "\n",
        "class trigger:\n",
        "    def __init__(self):\n",
        "        self._trigger = True\n",
        "\n",
        "    def __call__(self, e):\n",
        "        return self._trigger\n",
        "\n",
        "    def set(self, t):\n",
        "        self._trigger = t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf_CfL5CgoUm"
      },
      "outputs": [],
      "source": [
        "# quick check if everything works:\n",
        "env = gym.make('PandaPickAndPlace-v2',control_type='joints')\n",
        "obs = env.reset()\n",
        "env.robot.neutral_joint_values = np.array([0.00, 0.41, 0.00, -1.85, 3.14, 2.52, 0.79, 3.00, 3.00])\n",
        "print(obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgSmROJFulWY"
      },
      "source": [
        "## Some Utils\n",
        "\n",
        "We define some utils. Mostly copies of functions defined in [Create_and_save_demo](https://github.com/fellowship/Robotics-Simulator-Imitation-Learning/blob/main/Create_and_save_demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMQE2zRB33Ez"
      },
      "outputs": [],
      "source": [
        "panda_dataset_dir = 'PyBulletPerAct/pnp/PyBulletPerAct/data'\n",
        "\n",
        "def check_and_make(dir):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "check_and_make(panda_dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avjXcUrtfztn"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "from time import sleep\n",
        "\n",
        "def no_disconnect():\n",
        "  t = dt.datetime.now()\n",
        "  '''\n",
        "  function that ensures Colab pro doesn't disconnect.\n",
        "  '''\n",
        "  while True:\n",
        "      delta = dt.datetime.now()-t               \n",
        "      if delta.seconds >= 60:\n",
        "          print(\"1 Min\",dt.datetime.now())\n",
        "          t = dt.datetime.now()\n",
        "      sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS34KOF_R-ap"
      },
      "outputs": [],
      "source": [
        "def _normalize(pc):\n",
        "  return (pc - np.amin(pc)) / (np.amax(pc) - np.amin(pc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqkP22_rKoSL"
      },
      "outputs": [],
      "source": [
        "def ClipFloatValues(float_array, min_value, max_value):\n",
        "  '''\n",
        "  Clips values to the range [min_value, max_value].\n",
        "  First checks if any values are out of range and prints a message.\n",
        "  Then clips all values to the given range.\n",
        "  Args:\n",
        "    float_array: 2D array of floating point values to be clipped.\n",
        "    min_value: Minimum value of clip range.\n",
        "    max_value: Maximum value of clip range.\n",
        "  Returns:\n",
        "    The clipped array.\n",
        "  '''\n",
        "  if float_array.min() < min_value or float_array.max() > max_value:\n",
        "    float_array = np.clip(float_array, min_value, max_value)\n",
        "  return float_array\n",
        "\n",
        "DEFAULT_RGB_SCALE_FACTOR = 256000.0\n",
        "\n",
        "def float_array_to_rgb_image(float_array,\n",
        "                             scale_factor=DEFAULT_RGB_SCALE_FACTOR,\n",
        "                             drop_blue=False):\n",
        "  '''\n",
        "  Convert a floating point array of values to an RGB image.\n",
        "  Convert floating point values to a fixed point representation where\n",
        "  the RGB bytes represent a 24-bit integer.\n",
        "  R is the high order byte.\n",
        "  B is the low order byte.\n",
        "  The precision of the depth image is 1/256 mm.\n",
        "  Floating point values are scaled so that the integer values cover\n",
        "  the representable range of depths.\n",
        "  This image representation should only use lossless compression.\n",
        "  Args:\n",
        "    float_array: Input array of floating point depth values in meters.\n",
        "    scale_factor: Scale value applied to all float values.\n",
        "    drop_blue: Zero out the blue channel to improve compression, results in 1mm\n",
        "      precision depth values.\n",
        "  Returns:\n",
        "    24-bit RGB PIL Image object representing depth values.\n",
        "  '''\n",
        "  # Scale the floating point array.\n",
        "  scaled_array = np.floor(float_array * scale_factor + 0.5)\n",
        "\n",
        "  # Convert the array to integer type and clip to representable range.\n",
        "  min_inttype = 0\n",
        "  max_inttype = 2**24 - 1\n",
        "  scaled_array = ClipFloatValues(scaled_array, min_inttype, max_inttype)\n",
        "  int_array = scaled_array.astype(np.uint32)\n",
        "  # Calculate:\n",
        "  #   r = (f / 256) / 256  high byte\n",
        "  #   g = (f / 256) % 256  middle byte\n",
        "  #   b = f % 256          low byte\n",
        "  rg = np.divide(int_array, 256)\n",
        "  r = np.divide(rg, 256)\n",
        "  g = np.mod(rg, 256)\n",
        "  image_shape = int_array.shape\n",
        "  rgb_array = np.zeros((image_shape[0], image_shape[1], 3), dtype=np.uint8)\n",
        "  rgb_array[..., 0] = r\n",
        "  rgb_array[..., 1] = g\n",
        "  if not drop_blue:\n",
        "    # Calculate the blue channel and add it to the array.\n",
        "    b = np.mod(int_array, 256)\n",
        "    rgb_array[..., 2] = b\n",
        "  image_mode = 'RGB'\n",
        "  image = Image.fromarray(rgb_array, mode=image_mode)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFu7zoy5Eck6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.transform import Rotation as Rot\n",
        "\n",
        "def cvK2BulletP(K, w, h, near, far):\n",
        "    '''\n",
        "    cvKtoPulletP converts the K interinsic matrix as calibrated using Opencv\n",
        "    and ROS to the projection matrix used in openGL and Pybullet.\n",
        "\n",
        "    :param K:  OpenCV 3x3 camera intrinsic matrix\n",
        "    :param w:  Image width\n",
        "    :param h:  Image height\n",
        "    :near:     The nearest objects to be included in the render\n",
        "    :far:      The furthest objects to be included in the render\n",
        "    :return:   4x4 projection matrix as used in openGL and pybullet\n",
        "    '''\n",
        "    f_x = K[0,0]\n",
        "    f_y = K[1,1]\n",
        "    c_x = K[0,2]\n",
        "    c_y = K[1,2]\n",
        "    A = (near + far)/(near - far)\n",
        "    B = 2 * near * far / (near - far)\n",
        "\n",
        "    projection_matrix = [\n",
        "                        [2/w * f_x,  0,          (w - 2*c_x)/w,  0],\n",
        "                        [0,          2/h * f_y,  (2*c_y - h)/h,  0],\n",
        "                        [0,          0,          A,              B],\n",
        "                        [0,          0,          -1,             0]]\n",
        "    # The transpose is needed for respecting the array structure of the PyB\n",
        "    # and OpenGL\n",
        "    return np.array(projection_matrix).T.reshape(16).tolist()\n",
        "\n",
        "def BulletP2cvK(proj_mat, w, h):\n",
        "    '''\n",
        "    inverse of above cvK2BulletP function\n",
        "    '''\n",
        "    A,B = proj_mat[2,2],proj_mat[3,2]\n",
        "    n,f = B/(A-1), B/(A+1)\n",
        "    fx,fy = w*proj_mat[0,0]/2,h*proj_mat[1,1]/2\n",
        "    cx,cy = w*(1-proj_mat[2,0])/2, h*(proj_mat[2,1]+1)/2\n",
        "\n",
        "    return np.array([[fx,0,cx],[0,fy,cy],[0,0,1]]), n, f\n",
        "\n",
        "\n",
        "def cvPose2BulletView(Rt):\n",
        "    '''\n",
        "    cvPose2BulletView gets orientation and position as used \n",
        "    in ROS-TF and opencv and coverts it to the view matrix used \n",
        "    in openGL and pyBullet.\n",
        "    \n",
        "    :param Rt: 4x4 extrinsic matrix\n",
        "    :return:  4x4 view matrix as used in PyBullet and openGL\n",
        "    \n",
        "    '''\n",
        "\n",
        "    Tc = np.array([[1,   0,    0,  0],\n",
        "                   [0,  -1,    0,  0],\n",
        "                   [0,   0,   -1,  0],\n",
        "                   [0,   0,    0,  1]]).reshape(4,4)\n",
        "    \n",
        "    # pybullet pse is the inverse of the pose from the ROS-TF\n",
        "    T=Tc@np.linalg.inv(Rt)\n",
        "    # The transpose is needed for respecting the array structure of the OpenGL\n",
        "    viewMatrix = T.T.reshape(16)\n",
        "    return viewMatrix\n",
        "\n",
        "def BullettocvPose(X):\n",
        "    '''\n",
        "    Inverse function of cvPose2BulletView, i.e. returns extrinsic matrix from\n",
        "    PyBullet/OpenGL.\n",
        "    '''\n",
        "    Tc = np.array([[1,   0,    0,  0],\n",
        "                   [0,  -1,    0,  0],\n",
        "                   [0,   0,   -1,  0],\n",
        "                   [0,   0,    0,  1]]).reshape(4,4)\n",
        "    \n",
        "    T = np.linalg.inv(X.T)@Tc\n",
        "    \n",
        "    return T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLWyCRp8u0up"
      },
      "outputs": [],
      "source": [
        "# import Demo, Observation and ObservationElement classes \n",
        "# (see e.g. Create_and_save_demo)\n",
        "\n",
        "from rlbench.demo import Demo\n",
        "from rlbench.backend.observation import Observation\n",
        "from yarr.utils.observation_type import ObservationElement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6ez0PDD0jzU"
      },
      "outputs": [],
      "source": [
        "def _normalize_fn(pc,far,near):\n",
        "  return (pc-near)/(far-near)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO_eZ-d6UxUd"
      },
      "outputs": [],
      "source": [
        "IMAGE_FORMAT = '%d.png'\n",
        "\n",
        "LEFT_SHOULDER_RGB_FOLDER = 'left_shoulder_rgb'\n",
        "LEFT_SHOULDER_DEPTH_FOLDER = 'left_shoulder_depth'\n",
        "LEFT_SHOULDER_MASK_FOLDER = 'left_shoulder_mask'\n",
        "RIGHT_SHOULDER_RGB_FOLDER = 'right_shoulder_rgb'\n",
        "RIGHT_SHOULDER_DEPTH_FOLDER = 'right_shoulder_depth'\n",
        "RIGHT_SHOULDER_MASK_FOLDER = 'right_shoulder_mask'\n",
        "OVERHEAD_RGB_FOLDER = 'overhead_rgb'\n",
        "OVERHEAD_DEPTH_FOLDER = 'overhead_depth'\n",
        "OVERHEAD_MASK_FOLDER = 'overhead_mask'\n",
        "WRIST_RGB_FOLDER = 'wrist_rgb'\n",
        "WRIST_DEPTH_FOLDER = 'wrist_depth'\n",
        "WRIST_MASK_FOLDER = 'wrist_mask'\n",
        "FRONT_RGB_FOLDER = 'front_rgb'\n",
        "FRONT_DEPTH_FOLDER = 'front_depth'\n",
        "FRONT_MASK_FOLDER = 'front_mask'\n",
        "EPISODES_FOLDER = 'episodes'\n",
        "EPISODE_FOLDER = 'episode%d'\n",
        "VARIATIONS_FOLDER = 'variation%d'\n",
        "\n",
        "LOW_DIM_PICKLE = 'low_dim_obs.pkl'\n",
        "VARIATION_DESCRIPTIONS = 'variation_descriptions.pkl'\n",
        "\n",
        "DEPTH_SCALE = 2**24 - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRAKBn-7gjKc"
      },
      "source": [
        "# PerAct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCQorAMoc2Jh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "os.environ[\"DISPLAY\"] = \":0\"\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\" "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "TASK = 'pick and place'\n",
        "DATA_FOLDER ='PyBulletPerAct/'\n",
        "EPISODES_FOLDER = 'pnp/PyBulletPerAct/data'\n",
        "EPISODE_FOLDER = 'episode%d'\n",
        "CAMERAS = ['front', 'left_shoulder', 'right_shoulder', 'wrist']\n",
        "LOW_DIM_SIZE = 4   # {left_finger_joint, right_finger_joint, gripper_open, timestep}\n",
        "IMAGE_SIZE =  128  # 128x128 - if you want to use higher voxel resolutions like 200^3, you might want to regenerate the dataset with larger images\n",
        "\n",
        "# in my data we do not have variation_descriptions.pkl, this is something that should be implemented.\n",
        "# there are some details in the Create_and_save file.\n",
        "VARIATION_DESCRIPTIONS_PKL = 'variation_descriptions.pkl' # the pkl file that contains language goals for each demonstration\n",
        "EPISODE_LENGTH = 10 # max steps for agents\n",
        "DEMO_AUGMENTATION_EVERY_N = 10 # sample n-th frame in demo\n",
        "ROTATION_RESOLUTION = 5 # degree increments per axis\n",
        "\n",
        "# settings\n",
        "VOXEL_SIZES = [100] # 100x100x100 voxels\n",
        "NUM_LATENTS = 512 # PerceiverIO latents\n",
        "\n",
        "# I would suggest potentially optimizing th scene_bounds to have less empty space.\n",
        "SCENE_BOUNDS = [-0.5, -0.5, 0.0, 0.5, 0.5, 1.0] #the metric volume to be voxelized\n",
        "\n",
        "BATCH_SIZE = 1 \n",
        "NUM_DEMOS = 15 # total number of training demonstrations to use while training PerAct\n",
        "NUM_TEST = 5 # episodes to evaluate on"
      ],
      "metadata": {
        "id": "-Tl74dqejXlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZt5BxKgngP6"
      },
      "outputs": [],
      "source": [
        "data_path = os.path.join(DATA_FOLDER, EPISODES_FOLDER)\n",
        "\n",
        "train_replay_storage_dir = 'replay_train'\n",
        "if not os.path.exists(train_replay_storage_dir):\n",
        "  os.mkdir(train_replay_storage_dir)\n",
        "\n",
        "test_replay_storage_dir = 'replay_test'\n",
        "if not os.path.exists(test_replay_storage_dir):\n",
        "  os.mkdir(test_replay_storage_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG3WRcmfBKRB"
      },
      "source": [
        "## Data Loading & Preprocessing\n",
        "\n",
        "An expert demonstration recorded at ~20Hz contains 100s of individual timesteps in a sequence. Each timestep contains observations recorded from 4 calibrated cameras (front, left_shoulder, right_shoulder, and wrist) and other proprioception sensors. \"Calibrated\" means we know the extrinsics and intrinsics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GJ0KjUO-gaa"
      },
      "source": [
        "### Load Demo\n",
        "\n",
        "We now define functions that implement loading demos stored using [Create_and_save_demo](https://github.com/fellowship/Robotics-Simulator-Imitation-Learning/blob/main/Create_and_save_demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe7CKMUftdI7"
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "\n",
        "EPISODE_FOLDER = 'episode%d'\n",
        "\n",
        "CAMERA_FRONT = 'front'\n",
        "CAMERA_LS = 'left_shoulder'\n",
        "CAMERA_RS = 'right_shoulder'\n",
        "CAMERA_WRIST = 'wrist'\n",
        "CAMERAS = [CAMERA_FRONT, CAMERA_LS, CAMERA_RS, CAMERA_WRIST]\n",
        "\n",
        "IMAGE_RGB = 'rgb'\n",
        "IMAGE_DEPTH = 'depth'\n",
        "IMAGE_TYPES = [IMAGE_RGB, IMAGE_DEPTH]\n",
        "IMAGE_FORMAT  = '%d.png'\n",
        "LOW_DIM_PICKLE = 'low_dim_obs.pkl'\n",
        "VARIATION_NUMBER_PICKLE = 'variation_number.pkl'\n",
        "\n",
        "DEPTH_SCALE = 2**24 - 1\n",
        "\n",
        "index = 1\n",
        "episode_path = os.path.join(data_path, EPISODE_FOLDER % index)\n",
        "\n",
        "DEFAULT_RGB_SCALE_FACTOR = 256000.0\n",
        "\n",
        "DEFAULT_GRAY_SCALE_FACTOR = {np.uint8: 100.0,\n",
        "                             np.uint16: 1000.0,\n",
        "                             np.int32: DEFAULT_RGB_SCALE_FACTOR}\n",
        "\n",
        "\n",
        "def image_to_float_array(image, scale_factor=None):\n",
        "  '''\n",
        "  Converts image to float array (inverse of float_array_to_rgb_image)\n",
        "  Args:\n",
        "    :image: rgb image stored using float_array_to_rgb_image\n",
        "    :scale_factor: scale image\n",
        "  Returns:\n",
        "    np.array of rgb image.\n",
        "  '''\n",
        "  image_array = np.array(image)\n",
        "  image_dtype = image_array.dtype\n",
        "  image_shape = image_array.shape\n",
        "\n",
        "  channels = image_shape[2] if len(image_shape) > 2 else 1\n",
        "  assert 2 <= len(image_shape) <= 3\n",
        "  if channels == 3:\n",
        "    # RGB image needs to be converted to 24 bit integer.\n",
        "    float_array = np.sum(image_array * [65536, 256, 1], axis=2)\n",
        "    if scale_factor is None:\n",
        "      scale_factor = DEFAULT_RGB_SCALE_FACTOR\n",
        "  else:\n",
        "    if scale_factor is None:\n",
        "      scale_factor = DEFAULT_GRAY_SCALE_FACTOR[image_dtype.type]\n",
        "    float_array = image_array.astype(np.float32)\n",
        "  scaled_array = float_array / scale_factor\n",
        "  return scaled_array\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from pyrep.objects import VisionSensor\n",
        "\n",
        "# constants\n",
        "EPISODE_FOLDER = 'episode%d'\n",
        "\n",
        "CAMERA_FRONT = 'front'\n",
        "CAMERA_LS = 'left_shoulder'\n",
        "CAMERA_RS = 'right_shoulder'\n",
        "CAMERA_WRIST = 'wrist'\n",
        "CAMERAS = [CAMERA_FRONT, CAMERA_LS, CAMERA_RS, CAMERA_WRIST]\n",
        "\n",
        "IMAGE_RGB = 'rgb'\n",
        "IMAGE_DEPTH = 'depth'\n",
        "IMAGE_TYPES = [IMAGE_RGB, IMAGE_DEPTH]\n",
        "IMAGE_FORMAT  = '%d.png'\n",
        "LOW_DIM_PICKLE = 'low_dim_obs.pkl'\n",
        "VARIATION_NUMBER_PICKLE = 'variation_number.pkl'\n",
        "\n",
        "DEPTH_SCALE = 2**24 - 1\n",
        "\n",
        "# functions\n",
        "def get_stored_demo(data_path, index):\n",
        "  '''\n",
        "  function that loads demos from data_path and index as stored in the \n",
        "  conventions of Create_and_save_demo.\n",
        "  Args:\n",
        "    :data_path: path where data is stored\n",
        "    :index: specific episode to be loaded\n",
        "  \n",
        "  Returns:\n",
        "    demo stored as a list of Observations.\n",
        "  '''\n",
        "  episode_path = os.path.join(data_path, EPISODE_FOLDER % index)\n",
        "  \n",
        "  # low dim pickle file\n",
        "  with open(os.path.join(episode_path, LOW_DIM_PICKLE), 'rb') as f:\n",
        "    obs = pickle.load(f)\n",
        "\n",
        "  # variation number: if you want to load a text prompt associated with a demo,\n",
        "  # uncomment the following lines and modify Create_and_save_demo accordingly.\n",
        "  # with open(os.path.join(episode_path, VARIATION_NUMBER_PICKLE), 'rb') as f:\n",
        "  #  obs.variation_number = pickle.load(f)\n",
        "\n",
        "  obs.variation_number = 1\n",
        "  \n",
        "  num_steps = len(obs)\n",
        "  for i in range(num_steps):\n",
        "    obs[i].front_rgb = np.array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_FRONT, IMAGE_RGB), IMAGE_FORMAT % i)).convert('RGB'))\n",
        "    obs[i].left_shoulder_rgb = np.array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_LS, IMAGE_RGB), IMAGE_FORMAT % i)).convert('RGB'))\n",
        "    obs[i].right_shoulder_rgb = np.array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_RS, IMAGE_RGB), IMAGE_FORMAT % i)).convert('RGB'))\n",
        "    obs[i].wrist_rgb = np.array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_WRIST, IMAGE_RGB), IMAGE_FORMAT % i)).convert('RGB'))\n",
        "\n",
        "    obs[i].front_depth = image_to_float_array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_FRONT, IMAGE_DEPTH), IMAGE_FORMAT % i)), DEPTH_SCALE)\n",
        "    near = obs[i].misc['%s_camera_near' % (CAMERA_FRONT)]\n",
        "    far = obs[i].misc['%s_camera_far' % (CAMERA_FRONT)]\n",
        "    obs[i].front_depth = near + obs[i].front_depth * (far - near)\n",
        "\n",
        "    obs[i].left_shoulder_depth = image_to_float_array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_LS, IMAGE_DEPTH), IMAGE_FORMAT % i)), DEPTH_SCALE)\n",
        "    near = obs[i].misc['%s_camera_near' % (CAMERA_LS)]\n",
        "    far = obs[i].misc['%s_camera_far' % (CAMERA_LS)]\n",
        "    obs[i].left_shoulder_depth = near + obs[i].left_shoulder_depth * (far - near)\n",
        "\n",
        "    obs[i].right_shoulder_depth = image_to_float_array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_RS, IMAGE_DEPTH), IMAGE_FORMAT % i)), DEPTH_SCALE)\n",
        "    near = obs[i].misc['%s_camera_near' % (CAMERA_RS)]\n",
        "    far = obs[i].misc['%s_camera_far' % (CAMERA_RS)]\n",
        "    obs[i].right_shoulder_depth = near + obs[i].right_shoulder_depth * (far - near)\n",
        "\n",
        "    obs[i].wrist_depth = image_to_float_array(Image.open(os.path.join(episode_path, '%s_%s' % (CAMERA_WRIST, IMAGE_DEPTH), IMAGE_FORMAT % i)), DEPTH_SCALE)\n",
        "    near = obs[i].misc['%s_camera_near' % (CAMERA_WRIST)]\n",
        "    far = obs[i].misc['%s_camera_far' % (CAMERA_WRIST)]\n",
        "    obs[i].wrist_depth = near + obs[i].wrist_depth * (far - near)\n",
        "\n",
        "    obs[i].front_point_cloud = VisionSensor.pointcloud_from_depth_and_camera_params(obs[i].front_depth, \n",
        "                                                                                    obs[i].misc['front_camera_extrinsics'],\n",
        "                                                                                    obs[i].misc['front_camera_intrinsics'])\n",
        "    obs[i].left_shoulder_point_cloud = VisionSensor.pointcloud_from_depth_and_camera_params(obs[i].left_shoulder_depth, \n",
        "                                                                                            obs[i].misc['left_shoulder_camera_extrinsics'],\n",
        "                                                                                            obs[i].misc['left_shoulder_camera_intrinsics'])\n",
        "    obs[i].right_shoulder_point_cloud = VisionSensor.pointcloud_from_depth_and_camera_params(obs[i].right_shoulder_depth, \n",
        "                                                                                             obs[i].misc['right_shoulder_camera_extrinsics'],\n",
        "                                                                                             obs[i].misc['right_shoulder_camera_intrinsics'])\n",
        "    obs[i].wrist_point_cloud = VisionSensor.pointcloud_from_depth_and_camera_params(obs[i].wrist_depth, \n",
        "                                                                                           obs[i].misc['wrist_camera_extrinsics'],\n",
        "                                                                                           obs[i].misc['wrist_camera_intrinsics'])\n",
        "    \n",
        "  return obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMm2dQOz2vt2"
      },
      "outputs": [],
      "source": [
        "# Below is a way to get the point clouds from camera intrinsics/extrinsics in PyBullet\n",
        "# Not necessary for the following but I think it's worth keeping for people\n",
        "# to understand point clouds.\n",
        "# import pybullet as p\n",
        "# def get_point_cloud_from_Pybullet(width, height, view_matrix, proj_matrix):\n",
        "#     image_arr = p.getCameraImage(width=width, height=height, viewMatrix=view_matrix, projectionMatrix=proj_matrix)\n",
        "#     depth = image_arr[3]\n",
        "#     proj_matrix = np.asarray(proj_matrix).reshape([4, 4], order=\"F\")\n",
        "#     view_matrix = np.asarray(view_matrix).reshape([4, 4], order=\"F\")\n",
        "#     tran_pix_world = np.linalg.inv(np.matmul(proj_matrix, view_matrix))\n",
        "#     y, x = np.mgrid[-1:1:2 / height, -1:1:2 / width]\n",
        "#     y *= -1.\n",
        "#     x, y, z = x.reshape(-1), y.reshape(-1), depth.reshape(-1)\n",
        "#     h = np.ones_like(z)\n",
        "#     pixels = pixels[z < 0.99]\n",
        "#     pixels[:, 2] = 2 * pixels[:, 2] - 1\n",
        "#     points = np.matmul(tran_pix_world, pixels.T).T\n",
        "#     points /= points[:, 3: 4]\n",
        "#     points = points[:, :3]\n",
        "#     return points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPFjTUoH-n06"
      },
      "source": [
        "#### Some Tests\n",
        "\n",
        "Some basic tests to see whether demo import worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "DcqP4UMLVjyQ",
        "outputId": "8057fe43-3b09-4def-da48-6a4c06f5870b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo 1 | 172 total steps\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#from rlbench.utils import get_stored_demo\n",
        "from rlbench.backend.utils import extract_obs\n",
        "\n",
        "# what to visualize\n",
        "episode_idx_to_visualize = 1 # out of 10 demos\n",
        "ts = 50 # timestep out of total timesteps\n",
        "\n",
        "# get demo\n",
        "demo = get_stored_demo(data_path=data_path,\n",
        "                      index=episode_idx_to_visualize)     \n",
        "\n",
        "# extract obs at timestep                 \n",
        "obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
        "\n",
        "# total timesteps in demo\n",
        "print(f\"Demo {episode_idx_to_visualize} | {len(demo._observations)} total steps\\n\")\n",
        "\n",
        "# plot rgb and depth at timestep\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "rows, cols = 2, len(CAMERAS)\n",
        "\n",
        "plot_idx = 1\n",
        "for camera in CAMERAS:\n",
        "  # rgb\n",
        "  rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
        "  rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
        "  fig.add_subplot(rows, cols, plot_idx)\n",
        "  plt.imshow(rgb)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
        "\n",
        "  # depth\n",
        "  depth_name = \"%s_%s\" % (camera, 'depth')\n",
        "  depth = np.transpose(obs_dict[depth_name], (1, 2, 0)).reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
        "  fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
        "  plt.imshow(depth)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
        "\n",
        "  plot_idx += 1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYpQsneWpgY3",
        "outputId": "61d3e7ce-cece-4eff-a317-917358aab619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo 9 | 172 total steps\n"
          ]
        }
      ],
      "source": [
        "# Visualize demo 9 from wrist camera.\n",
        "\n",
        "episode_idx_to_visualize = 9\n",
        "demo_peract = get_stored_demo(data_path=data_path,\n",
        "                      index=episode_idx_to_visualize)     \n",
        "\n",
        "# total timesteps\n",
        "print(\"Demo %s | %s total steps\" % (episode_idx_to_visualize, len(demo._observations))) \n",
        "\n",
        "# use the heuristic to extract keyframes (aka keypoints)\n",
        "#episode_keypoints = _keypoint_discovery(demo)\n",
        "\n",
        "\n",
        "ff=[]\n",
        "for kp in range(len(demo_peract._observations)):\n",
        "  obs_dict = extract_obs(demo_peract._observations[kp], CAMERAS, t=kp)\n",
        "  rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
        "  ff.append(rgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0swQDx9ppHI"
      },
      "outputs": [],
      "source": [
        "# animate:\n",
        "\n",
        "display.HTML(display_anim(ff))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCyZcueDYvTq",
        "outputId": "d8d33e89-cdae-4ac5-b5a8-a9bc88079af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo 1 | 172 total steps\n"
          ]
        }
      ],
      "source": [
        "# Visualize demo 1 from front camera.\n",
        "\n",
        "episode_idx_to_visualize = 1\n",
        "demo = get_stored_demo(data_path=panda_dataset_dir,\n",
        "                      index=1)     \n",
        "\n",
        "# total timesteps\n",
        "print(\"Demo %s | %s total steps\" % (episode_idx_to_visualize, len(demo._observations))) \n",
        "\n",
        "# use the heuristic to extract keyframes (aka keypoints)\n",
        "#episode_keypoints = _keypoint_discovery(demo)\n",
        "\n",
        "\n",
        "ff=[]\n",
        "for kp in range(len(demo._observations)):\n",
        "  obs_dict = extract_obs(demo._observations[kp], CAMERAS, t=kp)\n",
        "  rgb = np.transpose(obs_dict['front_rgb'], (1, 2, 0))\n",
        "  ff.append(rgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGjPQUS_YxwI"
      },
      "outputs": [],
      "source": [
        "# animate:\n",
        "display.HTML(display_anim(ff))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx412PbcVMwZ"
      },
      "source": [
        "Basic check of whether the point cloud has been properly loaded:\n",
        "\n",
        "The picture below is supposed to show the intersection of the point cloud with the surfaces {x=0}, {y=0} and {z=0}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV_oHaaw4Fs6"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "rgb, depth, mask, misc = env.render(mode = 'rgb_array', width = 128, height = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "SoCob6TR-j8Z",
        "outputId": "3d824638-cf51-4f09-e116-47354e7472df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 128x128 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "dpi,fps=72,50\n",
        "test_pc = demo._observations[10].front_point_cloud\n",
        "\n",
        "y,x,_=np.where((test_pc<0.01)&(test_pc>-0.01))\n",
        "\n",
        "size = 1 # size of marker\n",
        "\n",
        "plt.figure(figsize=(test_pc.shape[1] / dpi, test_pc.shape[0] / dpi), dpi=dpi)\n",
        "plt.imshow((_normalize(test_pc)))\n",
        "\n",
        "#plt.imshow(img, cmap=\"gray\") # plot image\n",
        "plt.scatter(x, y, size, c=\"r\", marker=\"+\") # plot markers\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEqEV0mT3Tda"
      },
      "source": [
        "### Create Replay Buffer\n",
        "\n",
        "\n",
        "This replay buffer stores **<observation, language goal, keyframe action>** tuples sampled from demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghllJQShYW-S",
        "outputId": "393f6b62-3c4e-4083-b9f0-fb66f910ac1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-127c3d33fdab>:57: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  ReplayElement('demo', (), np.bool),\n"
          ]
        }
      ],
      "source": [
        "# Adapted from: https://github.com/stepjam/ARM/blob/main/arm/c2farm/launch_utils.py\n",
        "\n",
        "from yarr.utils.observation_type import ObservationElement\n",
        "from yarr.replay_buffer import ReplayElement, ReplayBuffer\n",
        "from yarr.replay_buffer.uniform_replay_buffer import UniformReplayBuffer\n",
        "\n",
        "\n",
        "def create_replay(batch_size: int, \n",
        "                  timesteps: int,\n",
        "                  save_dir: str, \n",
        "                  cameras: list, \n",
        "                  voxel_sizes, \n",
        "                  replay_size=3e5):\n",
        "\n",
        "    trans_indicies_size = 3 * len(voxel_sizes)\n",
        "    rot_and_grip_indicies_size = (3 + 1)\n",
        "    gripper_pose_size = 7\n",
        "    ignore_collisions_size = 1\n",
        "    max_token_seq_len = 77\n",
        "    lang_feat_dim = 1024\n",
        "    lang_emb_dim = 512\n",
        "\n",
        "    # low_dim_state\n",
        "    observation_elements = []\n",
        "    observation_elements.append(\n",
        "        ObservationElement('low_dim_state', (LOW_DIM_SIZE,), np.float32))\n",
        "    \n",
        "    # rgb, depth, point cloud, intrinsics, extrinsics\n",
        "    for cname in cameras:\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_rgb' % cname, (3, IMAGE_SIZE, IMAGE_SIZE,), np.float32))\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_depth' % cname, (1, IMAGE_SIZE, IMAGE_SIZE,), np.float32))\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_point_cloud' % cname, (3, IMAGE_SIZE, IMAGE_SIZE,), np.float32)) \n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_camera_extrinsics' % cname, (4, 4,), np.float32))\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_camera_intrinsics' % cname, (3, 3,), np.float32))\n",
        "\n",
        "    # discretized translation, discretized rotation, discrete ignore collision, 6-DoF gripper pose, and pre-trained language embeddings\n",
        "    observation_elements.extend([\n",
        "        ReplayElement('trans_action_indicies', (trans_indicies_size,), \n",
        "                      np.int32),\n",
        "        ReplayElement('rot_grip_action_indicies', (rot_and_grip_indicies_size,),\n",
        "                      np.int32),\n",
        "        ReplayElement('ignore_collisions', (ignore_collisions_size,),\n",
        "                      np.int32),\n",
        "        ReplayElement('gripper_pose', (gripper_pose_size,), \n",
        "                      np.float32),\n",
        "        ReplayElement('lang_goal_embs', (max_token_seq_len, lang_emb_dim,), # extracted from CLIP's language encoder\n",
        "                      np.float32),\n",
        "        ReplayElement('lang_goal', (1,), object), # language goal string for debugging and visualization\n",
        "    ])\n",
        "\n",
        "    extra_replay_elements = [\n",
        "        ReplayElement('demo', (), np.bool),\n",
        "    ]\n",
        "\n",
        "    replay_buffer = UniformReplayBuffer( # all tuples in the buffer have equal sample weighting\n",
        "        save_dir=save_dir,\n",
        "        batch_size=batch_size,\n",
        "        timesteps=timesteps,\n",
        "        replay_capacity=int(replay_size),\n",
        "        action_shape=(8,), # 3 translation + 4 rotation quaternion + 1 gripper open\n",
        "        action_dtype=np.float32,\n",
        "        reward_shape=(),\n",
        "        reward_dtype=np.float32,\n",
        "        update_horizon=1,\n",
        "        observation_elements=observation_elements,\n",
        "        extra_replay_elements=extra_replay_elements\n",
        "    )\n",
        "    return replay_buffer\n",
        "\n",
        "train_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
        "                                    timesteps=1,\n",
        "                                    save_dir=train_replay_storage_dir,\n",
        "                                    cameras=CAMERAS,\n",
        "                                    voxel_sizes=VOXEL_SIZES)\n",
        "\n",
        "test_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
        "                                   timesteps=1,\n",
        "                                   save_dir=test_replay_storage_dir,\n",
        "                                   cameras=CAMERAS,\n",
        "                                   voxel_sizes=VOXEL_SIZES)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GeqBQG03vTG"
      },
      "source": [
        "### Fill Replay with Demos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Spr_Tk0Z0tP"
      },
      "source": [
        "#### Keyframe Extraction\n",
        "\n",
        "Functions that extract keypoints from a demo. Keypoints are characterized by the following:\n",
        "\n",
        "1.   Joint velocities are close to zero (change tolerance delta if you want to adjust sensitivity)\n",
        "2.   Change in gripper open status.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsyOYB_v6VNk"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/stepjam/ARM/blob/main/arm/demo_loading_utils.py\n",
        "\n",
        "from rlbench.demo import Demo\n",
        "from typing import List\n",
        "\n",
        "def _is_stopped(demo, i, obs, stopped_buffer, delta=0.03):\n",
        "    next_is_not_final = i == (len(demo) - 2)\n",
        "    gripper_state_no_change = (\n",
        "            i < (len(demo) - 2) and\n",
        "            (obs.gripper_open == demo[i + 1].gripper_open and\n",
        "             obs.gripper_open == demo[i - 1].gripper_open and\n",
        "             demo[i - 2].gripper_open == demo[i - 1].gripper_open))\n",
        "    small_delta = np.allclose(obs.joint_velocities, 0, atol=delta)\n",
        "    stopped = (stopped_buffer <= 0 and small_delta and\n",
        "               (not next_is_not_final) and gripper_state_no_change)\n",
        "    return stopped\n",
        "\n",
        "def _keypoint_discovery(demo: Demo,\n",
        "                        stopping_delta=0.03) -> List[int]:\n",
        "    episode_keypoints = []\n",
        "    prev_gripper_open = demo[0].gripper_open\n",
        "    stopped_buffer = 0\n",
        "    for i, obs in enumerate(demo):\n",
        "        stopped = _is_stopped(demo, i, obs, stopped_buffer, stopping_delta)\n",
        "        stopped_buffer = 4 if stopped else stopped_buffer - 1\n",
        "        # if change in gripper, or end of episode.\n",
        "        last = i == (len(demo) - 1)\n",
        "        if i != 0 and (obs.gripper_open != prev_gripper_open or\n",
        "                        last or stopped):\n",
        "            episode_keypoints.append(i)\n",
        "        prev_gripper_open = obs.gripper_open\n",
        "    if len(episode_keypoints) > 1 and (episode_keypoints[-1] - 1) == \\\n",
        "            episode_keypoints[-2]:\n",
        "        episode_keypoints.pop(-2)\n",
        "    print('Found %d keypoints.' % len(episode_keypoints), episode_keypoints)\n",
        "    return episode_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdipWAdGn3Pd"
      },
      "source": [
        "Let's take a look at what these keyframe actions look like. \n",
        "\n",
        "**Notice:** If there are too many/too few keyframes for the type of demos loaded, you can adjust the sensitivity in the keypoint extraction method above. I don't think it's necessarily a problem to have a more fine-grained approach, but it might be less efficient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHOsZUlPk-0z"
      },
      "outputs": [],
      "source": [
        "episode_idx_to_visualize = 1\n",
        "demo = get_stored_demo(data_path=data_path,\n",
        "                      index=episode_idx_to_visualize)     \n",
        "\n",
        "# total timesteps\n",
        "print(\"Demo %s | %s total steps\" % (episode_idx_to_visualize, len(demo._observations))) \n",
        "\n",
        "# use the heuristic to extract keyframes (aka keypoints)\n",
        "episode_keypoints = _keypoint_discovery(demo)\n",
        "\n",
        "# visualize rgb observations from these keyframes\n",
        "for kp_idx, kp in enumerate(episode_keypoints):\n",
        "    obs_dict = extract_obs(demo._observations[kp], CAMERAS, t=kp)\n",
        "\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    rgb_name = \"front_rgb\"\n",
        "    rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
        "    plt.imshow(rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"front_rgb | step %s | keypoint %s \" % (kp, kp_idx))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlvQXw-7oQc5"
      },
      "source": [
        "Notice that the motion-planner used to generate demonstrations might take various paths to execute the \"opening\" motion, but all paths strictly pass through these bottleneck poses, since that's how the expert demonstrations were collected in RLBench. This essentially circuments the issue of training directly on randomized motion paths from sampling-based motion planners, which can be quite noisy to learn from for end-to-end methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3NW3dKQpx98"
      },
      "source": [
        "#### Fill Replay\n",
        "\n",
        "Some helper functions for filling the replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db_0S-7UpN1d"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "import arm.utils as utils\n",
        "\n",
        "from rlbench.backend.observation import Observation\n",
        "from yarr.replay_buffer.replay_buffer import ReplayBuffer\n",
        "\n",
        "# discretize translation, rotation, gripper open, and ignore collision actions\n",
        "def _get_action(\n",
        "        obs_tp1: Observation,\n",
        "        obs_tm1: Observation,\n",
        "        rlbench_scene_bounds: List[float], # metric 3D bounds of the scene\n",
        "        voxel_sizes: List[int],\n",
        "        rotation_resolution: int,\n",
        "        crop_augmentation: bool):\n",
        "    quat = utils.normalize_quaternion(obs_tp1.gripper_pose[3:])\n",
        "    if quat[-1] < 0:\n",
        "        quat = -quat\n",
        "    disc_rot = utils.quaternion_to_discrete_euler(quat, rotation_resolution)\n",
        "    attention_coordinate = obs_tp1.gripper_pose[:3]\n",
        "    trans_indicies, attention_coordinates = [], []\n",
        "    bounds = np.array(rlbench_scene_bounds)\n",
        "    ignore_collisions = int(obs_tm1.ignore_collisions)\n",
        "    for depth, vox_size in enumerate(voxel_sizes): # only single voxelization-level is used in PerAct\n",
        "        index = utils.point_to_voxel_index(\n",
        "            obs_tp1.gripper_pose[:3], vox_size, bounds)\n",
        "        trans_indicies.extend(index.tolist())\n",
        "        res = (bounds[3:] - bounds[:3]) / vox_size\n",
        "        attention_coordinate = bounds[:3] + res * index\n",
        "        attention_coordinates.append(attention_coordinate)\n",
        "\n",
        "    rot_and_grip_indicies = disc_rot.tolist()\n",
        "    grip = float(obs_tp1.gripper_open)\n",
        "    rot_and_grip_indicies.extend([int(obs_tp1.gripper_open)])\n",
        "    return trans_indicies, rot_and_grip_indicies, ignore_collisions, np.concatenate(\n",
        "        [obs_tp1.gripper_pose, np.array([grip])]), attention_coordinates\n",
        "\n",
        "# extract CLIP language features for goal string\n",
        "def _clip_encode_text(clip_model, text):\n",
        "    x = clip_model.token_embedding(text).type(clip_model.dtype)  # [batch_size, n_ctx, d_model]\n",
        "\n",
        "    x = x + clip_model.positional_embedding.type(clip_model.dtype)\n",
        "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "    x = clip_model.transformer(x)\n",
        "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "    x = clip_model.ln_final(x).type(clip_model.dtype)\n",
        "\n",
        "    emb = x.clone()\n",
        "    x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ clip_model.text_projection\n",
        "\n",
        "    return x, emb\n",
        "\n",
        "# add individual data points to replay\n",
        "def _add_keypoints_to_replay(\n",
        "        replay: ReplayBuffer,\n",
        "        inital_obs: Observation,\n",
        "        demo: Demo,\n",
        "        episode_keypoints: List[int],\n",
        "        cameras: List[str],\n",
        "        rlbench_scene_bounds: List[float],   \n",
        "        voxel_sizes: List[int],\n",
        "        rotation_resolution: int,\n",
        "        crop_augmentation: bool,\n",
        "        description: str = '',\n",
        "        clip_model = None,\n",
        "        device = 'cpu'):\n",
        "    prev_action = None\n",
        "    obs = inital_obs\n",
        "    for k, keypoint in enumerate(episode_keypoints):\n",
        "        obs_tp1 = demo[keypoint]\n",
        "        obs_tm1 = demo[max(0, keypoint - 1)]\n",
        "        trans_indicies, rot_grip_indicies, ignore_collisions, action, attention_coordinates = _get_action(\n",
        "            obs_tp1, obs_tm1, rlbench_scene_bounds, voxel_sizes,\n",
        "            rotation_resolution, crop_augmentation)\n",
        "\n",
        "        terminal = (k == len(episode_keypoints) - 1)\n",
        "        reward = float(terminal) * 1.0 if terminal else 0\n",
        "\n",
        "        obs_dict = extract_obs(obs, CAMERAS, t=k, prev_action=prev_action)\n",
        "        tokens = clip.tokenize([description]).numpy()\n",
        "        token_tensor = torch.from_numpy(tokens).to(device)\n",
        "        lang_feats, lang_embs = _clip_encode_text(clip_model, token_tensor)\n",
        "        obs_dict['lang_goal_embs'] = lang_embs[0].float().detach().cpu().numpy()\n",
        "\n",
        "        prev_action = np.copy(action)\n",
        "\n",
        "        others = {'demo': True}\n",
        "        final_obs = {\n",
        "            'trans_action_indicies': trans_indicies,\n",
        "            'rot_grip_action_indicies': rot_grip_indicies,\n",
        "            'gripper_pose': obs_tp1.gripper_pose,\n",
        "            'lang_goal': np.array([description], dtype=object),\n",
        "        }\n",
        "\n",
        "        others.update(final_obs)\n",
        "        others.update(obs_dict)\n",
        "\n",
        "        timeout = False\n",
        "        replay.add(action, reward, terminal, timeout, **others)\n",
        "        obs = obs_tp1  \n",
        "\n",
        "    # final step\n",
        "    obs_dict_tp1 = extract_obs(obs_tp1, CAMERAS, t=k + 1, prev_action=prev_action)\n",
        "    obs_dict_tp1['lang_goal_embs'] = lang_embs[0].float().detach().cpu().numpy()\n",
        "\n",
        "    obs_dict_tp1.pop('wrist_world_to_cam', None)\n",
        "    obs_dict_tp1.update(final_obs)\n",
        "    replay.add_final(**obs_dict_tp1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JoelGN-_a5O"
      },
      "source": [
        "Finally fill the replay buffer.\n",
        "\n",
        "It is important to change the description/model text-prompt based on the specific tasks we're training PyBulletPerAct on. \n",
        "\n",
        "One can implement this automatically -- say by adding a discription key in obs.misc in create_and_save_demo or by saving a bespoke description pickle file in the episode folder (see below). I've not implemented this as the tasks are not that diverse at this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwFMLOG0pMEq"
      },
      "outputs": [],
      "source": [
        "def fill_replay(replay: ReplayBuffer,\n",
        "                start_idx: int,\n",
        "                num_demos: int,\n",
        "                demo_augmentation: bool,\n",
        "                demo_augmentation_every_n: int,\n",
        "                cameras: List[str],\n",
        "                rlbench_scene_bounds: List[float],  # AKA: DEPTH0_BOUNDS\n",
        "                voxel_sizes: List[int],\n",
        "                rotation_resolution: int,\n",
        "                crop_augmentation: bool,\n",
        "                clip_model = None,\n",
        "                device = 'cpu'):\n",
        "    print('Filling replay ...')\n",
        "    for d_idx in range(start_idx, start_idx+num_demos):\n",
        "        print(\"Filling demo %d\" % d_idx)\n",
        "        demo = get_stored_demo(data_path=data_path,\n",
        "                               index=d_idx)\n",
        "\n",
        "        # ---------------------- \n",
        "        # ---------------------- \n",
        "        # need to explicitly change this for different tasks or automate:\n",
        "\n",
        "        # get language goal from disk\n",
        "        #varation_descs_pkl_file = os.path.join(data_path, EPISODE_FOLDER % d_idx, VARIATION_DESCRIPTIONS_PKL)\n",
        "        #with open(varation_descs_pkl_file, 'rb') as f:\n",
        "        #  descs = pickle.load(f)\n",
        "        descs = 'pick up blue cup and place it to the top left of the red one' \n",
        "        # ---------------------- \n",
        "        # ---------------------- \n",
        "        \n",
        "        # extract keypoints\n",
        "        episode_keypoints = _keypoint_discovery(demo)\n",
        "\n",
        "        for i in range(len(demo) - 1):\n",
        "            if not demo_augmentation and i > 0:\n",
        "                break\n",
        "            if i % demo_augmentation_every_n != 0: # choose only every n-th frame\n",
        "                continue\n",
        "\n",
        "            obs = demo[i]\n",
        "            desc = descs[0]\n",
        "            # if our starting point is past one of the keypoints, then remove it\n",
        "            while len(episode_keypoints) > 0 and i >= episode_keypoints[0]:\n",
        "                episode_keypoints = episode_keypoints[1:]\n",
        "            if len(episode_keypoints) == 0:\n",
        "                break\n",
        "            _add_keypoints_to_replay(\n",
        "                replay, obs, demo, episode_keypoints, cameras,\n",
        "                rlbench_scene_bounds, voxel_sizes,\n",
        "                rotation_resolution, crop_augmentation, description=desc,\n",
        "                clip_model=clip_model, device=device)\n",
        "    print('Replay filled with demos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lPNE8Pjsmr3"
      },
      "source": [
        "Load a [pre-trained CLIP model](https://arxiv.org/abs/2103.00020) to extract language features. You can probably swap this with other language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrO9aVEXsRps"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"RN50\", device=device) # CLIP-ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmKmx2U48vSJ"
      },
      "outputs": [],
      "source": [
        "from yarr.replay_buffer.wrappers.pytorch_replay_buffer import PyTorchReplayBuffer\n",
        "\n",
        "print(\"-- Train Buffer --\")\n",
        "fill_replay(replay=train_replay_buffer,\n",
        "            start_idx=0,\n",
        "            num_demos= NUM_DEMOS,\n",
        "            demo_augmentation=True,\n",
        "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
        "            cameras=CAMERAS,\n",
        "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
        "            voxel_sizes=VOXEL_SIZES,\n",
        "            rotation_resolution=ROTATION_RESOLUTION,\n",
        "            crop_augmentation=False,\n",
        "            clip_model=clip_model,\n",
        "            device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5aeRu7_flJH"
      },
      "outputs": [],
      "source": [
        "print(\"-- Test Buffer --\")\n",
        "fill_replay(replay=test_replay_buffer,\n",
        "            start_idx= NUM_DEMOS,\n",
        "            num_demos = NUM_TEST,\n",
        "            demo_augmentation=True,\n",
        "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
        "            cameras=CAMERAS,\n",
        "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
        "            voxel_sizes=VOXEL_SIZES,\n",
        "            rotation_resolution=ROTATION_RESOLUTION,\n",
        "            crop_augmentation=False,\n",
        "            clip_model=clip_model,\n",
        "            device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXI6SMrwbwEt"
      },
      "outputs": [],
      "source": [
        "# wrap buffer with PyTorch dataset and make iterator\n",
        "train_wrapped_replay = PyTorchReplayBuffer(train_replay_buffer)\n",
        "train_dataset = train_wrapped_replay.dataset()\n",
        "train_data_iter = iter(train_dataset)\n",
        "\n",
        "test_wrapped_replay = PyTorchReplayBuffer(test_replay_buffer)\n",
        "test_dataset = test_wrapped_replay.dataset()\n",
        "test_data_iter = iter(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2dFpk67pKrj"
      },
      "outputs": [],
      "source": [
        "# delete the CLIP model since we have already extracted language features\n",
        "del clip_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVbNjqUAxFv"
      },
      "source": [
        "## Training PerAct\n",
        "\n",
        "### Voxelization\n",
        "\n",
        "Now we define a class for voxelizing calibrated RGB-D observations following [C2FARM \\(James et al.\\)](https://arxiv.org/pdf/2106.12534.pdf)\n",
        "\n",
        "The input to the voxelizer is:\n",
        "- Flattened RGB images\n",
        "- Flattened global-coordinate point clouds\n",
        "- Scene bounds in metric units that specify the volume to be voxelized\n",
        "\n",
        "The output is a 10-dimensional voxel grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7UFToUQAr1t"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/stepjam/ARM/blob/main/arm/c2farm/voxel_grid.py\n",
        "\n",
        "from functools import reduce as funtool_reduce\n",
        "from operator import mul\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MIN_DENOMINATOR = 1e-12\n",
        "INCLUDE_PER_VOXEL_COORD = False\n",
        "\n",
        "\n",
        "class VoxelGrid(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 coord_bounds,\n",
        "                 voxel_size: int,\n",
        "                 device,\n",
        "                 batch_size,\n",
        "                 feature_size, \n",
        "                 max_num_coords: int,):\n",
        "        super(VoxelGrid, self).__init__()\n",
        "        self._device = device\n",
        "        self._voxel_size = voxel_size\n",
        "        self._voxel_shape = [voxel_size] * 3\n",
        "        self._voxel_d = float(self._voxel_shape[-1])\n",
        "        self._voxel_feature_size = 4 + feature_size\n",
        "        self._voxel_shape_spec = torch.tensor(self._voxel_shape,\n",
        "                                              device=device).unsqueeze(\n",
        "            0) + 2  # +2 because we crop the edges.\n",
        "        self._coord_bounds = torch.tensor(coord_bounds, dtype=torch.float,\n",
        "                                          device=device).unsqueeze(0)\n",
        "        max_dims = self._voxel_shape_spec[0]\n",
        "        self._total_dims_list = torch.cat(\n",
        "            [torch.tensor([batch_size], device=device), max_dims,\n",
        "             torch.tensor([4 + feature_size], device=device)], -1).tolist()\n",
        "        self._ones_max_coords = torch.ones((batch_size, max_num_coords, 1),\n",
        "                                           device=device)\n",
        "        self._num_coords = max_num_coords\n",
        "\n",
        "        shape = self._total_dims_list\n",
        "\n",
        "        self._result_dim_sizes = torch.tensor(\n",
        "            [funtool_reduce(mul, shape[i + 1:], 1) for i in range(len(shape) - 1)] + [\n",
        "                1], device=device)\n",
        "        flat_result_size = funtool_reduce(mul, shape, 1)\n",
        "\n",
        "        self._initial_val = torch.tensor(0, dtype=torch.float,\n",
        "                                         device=device)\n",
        "        self._flat_output = torch.ones(flat_result_size, dtype=torch.float,\n",
        "                                       device=device) * self._initial_val\n",
        "        self._arange_to_max_coords = torch.arange(4 + feature_size,\n",
        "                                                  device=device)\n",
        "        self._flat_zeros = torch.zeros(flat_result_size, dtype=torch.float,\n",
        "                                       device=device)\n",
        "\n",
        "        self._const_1 = torch.tensor(1.0, device=device)\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        # Coordinate Bounds:\n",
        "        self._bb_mins = self._coord_bounds[..., 0:3]\n",
        "        bb_maxs = self._coord_bounds[..., 3:6]\n",
        "        bb_ranges = bb_maxs - self._bb_mins\n",
        "        # get voxel dimensions. 'DIMS' mode\n",
        "        self._dims = dims = self._voxel_shape_spec.int()\n",
        "        self._dims_orig = dims_orig = self._voxel_shape_spec.int() - 2\n",
        "        self._dims_m_one = (dims - 1).int()\n",
        "        # BS x 1 x 3\n",
        "        self._res = bb_ranges / (dims_orig.float() + MIN_DENOMINATOR)\n",
        "        self._res_minis_2 = bb_ranges / (dims.float() - 2 + MIN_DENOMINATOR)\n",
        "\n",
        "        self._voxel_indicy_denmominator = self._res + MIN_DENOMINATOR\n",
        "        self._dims_m_one_zeros = torch.zeros_like(self._dims_m_one)\n",
        "\n",
        "        batch_indices = torch.arange(self._batch_size, dtype=torch.int,\n",
        "                                     device=device).view(self._batch_size, 1, 1)\n",
        "        self._tiled_batch_indices = batch_indices.repeat(\n",
        "            [1, self._num_coords, 1])\n",
        "\n",
        "        w = self._voxel_shape[0] + 2\n",
        "        arange = torch.arange(0, w, dtype=torch.float, device=device)\n",
        "        self._index_grid = torch.cat([\n",
        "            arange.view(w, 1, 1, 1).repeat([1, w, w, 1]),\n",
        "            arange.view(1, w, 1, 1).repeat([w, 1, w, 1]),\n",
        "            arange.view(1, 1, w, 1).repeat([w, w, 1, 1])], dim=-1).unsqueeze(\n",
        "            0).repeat([self._batch_size, 1, 1, 1, 1])\n",
        "\n",
        "    def _broadcast(self, src: torch.Tensor, other: torch.Tensor, dim: int):\n",
        "        if dim < 0:\n",
        "            dim = other.dim() + dim\n",
        "        if src.dim() == 1:\n",
        "            for _ in range(0, dim):\n",
        "                src = src.unsqueeze(0)\n",
        "        for _ in range(src.dim(), other.dim()):\n",
        "            src = src.unsqueeze(-1)\n",
        "        src = src.expand_as(other)\n",
        "        return src\n",
        "\n",
        "    def _scatter_mean(self, src: torch.Tensor, index: torch.Tensor, out: torch.Tensor,\n",
        "                      dim: int = -1):\n",
        "        out = out.scatter_add_(dim, index, src)\n",
        "\n",
        "        index_dim = dim\n",
        "        if index_dim < 0:\n",
        "            index_dim = index_dim + src.dim()\n",
        "        if index.dim() <= index_dim:\n",
        "            index_dim = index.dim() - 1\n",
        "\n",
        "        ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n",
        "        out_count = torch.zeros(out.size(), dtype=out.dtype, device=out.device)\n",
        "        out_count = out_count.scatter_add_(index_dim, index, ones)\n",
        "        out_count.clamp_(1)\n",
        "        count = self._broadcast(out_count, out, dim)\n",
        "        if torch.is_floating_point(out):\n",
        "            out.true_divide_(count)\n",
        "        else:\n",
        "            out.floor_divide_(count)\n",
        "        return out\n",
        "\n",
        "    def _scatter_nd(self, indices, updates):\n",
        "        indices_shape = indices.shape\n",
        "        num_index_dims = indices_shape[-1]\n",
        "        flat_updates = updates.view((-1,))\n",
        "        indices_scales = self._result_dim_sizes[0:num_index_dims].view(\n",
        "            [1] * (len(indices_shape) - 1) + [num_index_dims])\n",
        "        indices_for_flat_tiled = ((indices * indices_scales).sum(\n",
        "            dim=-1, keepdims=True)).view(-1, 1).repeat(\n",
        "            *[1, self._voxel_feature_size])\n",
        "\n",
        "        implicit_indices = self._arange_to_max_coords[\n",
        "                           :self._voxel_feature_size].unsqueeze(0).repeat(\n",
        "            *[indices_for_flat_tiled.shape[0], 1])\n",
        "        indices_for_flat = indices_for_flat_tiled + implicit_indices\n",
        "        flat_indices_for_flat = indices_for_flat.view((-1,)).long()\n",
        "\n",
        "        flat_scatter = self._scatter_mean(\n",
        "            flat_updates, flat_indices_for_flat,\n",
        "            out=torch.zeros_like(self._flat_output))\n",
        "        return flat_scatter.view(self._total_dims_list)\n",
        "\n",
        "    def coords_to_bounding_voxel_grid(self, coords, coord_features=None,\n",
        "                                      coord_bounds=None):\n",
        "        voxel_indicy_denmominator = self._voxel_indicy_denmominator\n",
        "        res, bb_mins = self._res, self._bb_mins\n",
        "        if coord_bounds is not None:\n",
        "            bb_mins = coord_bounds[..., 0:3]\n",
        "            bb_maxs = coord_bounds[..., 3:6]\n",
        "            bb_ranges = bb_maxs - bb_mins\n",
        "            res = bb_ranges / (self._dims_orig.float() + MIN_DENOMINATOR)\n",
        "            voxel_indicy_denmominator = res + MIN_DENOMINATOR\n",
        "\n",
        "        bb_mins_shifted = bb_mins - res  # shift back by one\n",
        "        floor = torch.floor(\n",
        "            (coords - bb_mins_shifted.unsqueeze(1)) / voxel_indicy_denmominator.unsqueeze(1)).int()\n",
        "        voxel_indices = torch.min(floor, self._dims_m_one)\n",
        "        voxel_indices = torch.max(voxel_indices, self._dims_m_one_zeros)\n",
        "\n",
        "        # global-coordinate point cloud (x, y, z) \n",
        "        voxel_values = coords \n",
        "\n",
        "        # rgb values (R, G, B)\n",
        "        if coord_features is not None:\n",
        "            voxel_values = torch.cat([voxel_values, coord_features], -1) # concat rgb values (B, 128, 128, 3)\n",
        "\n",
        "        # coordinates to aggregate over\n",
        "        _, num_coords, _ = voxel_indices.shape\n",
        "        all_indices = torch.cat([\n",
        "            self._tiled_batch_indices[:, :num_coords], voxel_indices], -1)\n",
        "\n",
        "        # max coordinates \n",
        "        voxel_values_pruned_flat = torch.cat(\n",
        "            [voxel_values, self._ones_max_coords[:, :num_coords]], -1)\n",
        "\n",
        "        # aggregate across camera views\n",
        "        scattered = self._scatter_nd(\n",
        "            all_indices.view([-1, 1 + 3]),\n",
        "            voxel_values_pruned_flat.view(-1, self._voxel_feature_size))\n",
        "\n",
        "        vox = scattered[:, 1:-1, 1:-1, 1:-1]\n",
        "        if INCLUDE_PER_VOXEL_COORD:\n",
        "            res_expanded = res.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "            res_centre = (res_expanded * self._index_grid) + res_expanded / 2.0\n",
        "            coord_positions = (res_centre + bb_mins_shifted.unsqueeze(\n",
        "                1).unsqueeze(1).unsqueeze(1))[:, 1:-1, 1:-1, 1:-1]\n",
        "            vox = torch.cat([vox[..., :-1], coord_positions, vox[..., -1:]], -1)\n",
        "\n",
        "        # occupied value\n",
        "        occupied = (vox[..., -1:] > 0).float()\n",
        "        vox = torch.cat([\n",
        "            vox[..., :-1], occupied], -1)\n",
        "        \n",
        "        # hard voxel-location position encoding\n",
        "        return torch.cat(\n",
        "           [vox[..., :-1], self._index_grid[:, :-2, :-2, :-2] / self._voxel_d,\n",
        "            vox[..., -1:]], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nymM14ruQxiR"
      },
      "source": [
        "#### Quick Test:\n",
        "\n",
        "Some helper functions to normalize and format RGB and pointcloud input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv9Yplc8LBGt"
      },
      "outputs": [],
      "source": [
        "from arm.utils import stack_on_channel\n",
        "\n",
        "def _norm_rgb(x):\n",
        "    return (x.float() / 255.0) * 2.0 - 1.0\n",
        "\n",
        "def _preprocess_inputs(replay_sample):\n",
        "    obs, pcds = [], []\n",
        "    for n in CAMERAS:\n",
        "        rgb = stack_on_channel(replay_sample['%s_rgb' % n])\n",
        "        pcd = stack_on_channel(replay_sample['%s_point_cloud' % n])\n",
        "        \n",
        "        rgb = _norm_rgb(rgb)\n",
        "\n",
        "        obs.append([rgb, pcd]) # obs contains both rgb and pointcloud (used in ARM for other baselines)\n",
        "        pcds.append(pcd) # only pointcloud\n",
        "    return obs, pcds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrP8sMevRaZv"
      },
      "source": [
        "The rgb and pointcloud inputs have to be flattened before feeding them into the voxelizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIlYMxU-Jgn5"
      },
      "outputs": [],
      "source": [
        "from arm.utils import visualise_voxel\n",
        "\n",
        "# initialize voxelizer\n",
        "vox_grid = VoxelGrid(\n",
        "    coord_bounds=SCENE_BOUNDS,\n",
        "    voxel_size=VOXEL_SIZES[0],\n",
        "    device=device,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    feature_size=3,\n",
        "    max_num_coords=np.prod([IMAGE_SIZE, IMAGE_SIZE]) * len(CAMERAS),\n",
        ")\n",
        "\n",
        "# sample from dataset\n",
        "batch = next(train_data_iter)\n",
        "lang_goal = batch['lang_goal'][0][0][0]\n",
        "batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
        "\n",
        "# preprocess observations\n",
        "obs, pcds = _preprocess_inputs(batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeLRSZfMcyCD"
      },
      "outputs": [],
      "source": [
        "# Quick check:\n",
        "\n",
        "#for i in range(len(obs)):\n",
        "#  ff=[np.transpose(obs[i][0][0].cpu().detach().numpy(),axes=(1,2,0))]\n",
        "#  display.HTML(display_anim(ff));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpIN8FIV1XTv"
      },
      "outputs": [],
      "source": [
        "# flatten observations\n",
        "bs = obs[0][0].shape[0]\n",
        "pcd_flat = torch.cat([p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in pcds], 1)\n",
        "\n",
        "image_features = [o[0] for o in obs]\n",
        "feat_size = image_features[0].shape[1]\n",
        "flat_imag_features = torch.cat(\n",
        "    [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in image_features], 1)\n",
        "\n",
        "# tensorize scene bounds\n",
        "bounds = torch.tensor(SCENE_BOUNDS, device=device).unsqueeze(0)\n",
        "\n",
        "# voxelize!\n",
        "voxel_grid = vox_grid.coords_to_bounding_voxel_grid(pcd_flat, \n",
        "                                                    coord_features=flat_imag_features, \n",
        "                                                    coord_bounds=bounds)\n",
        "\n",
        "# swap to channels fist\n",
        "vis_voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach().cpu().numpy()\n",
        "\n",
        "# expert action voxel indicies\n",
        "vis_gt_coord = batch['trans_action_indicies'][:, -1, :3].int().detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYm47FRxmgyQ"
      },
      "outputs": [],
      "source": [
        "# render voxel grid with expert action (blue)\n",
        "#@markdown #### Show voxel grid and expert action (blue)\n",
        "#@markdown Adjust `rotation_amount` to change the camera yaw angle for rendering.\n",
        "\n",
        "rotation_amount = -180 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
        "rendered_img = visualise_voxel(vis_voxel_grid[0],\n",
        "                               None,\n",
        "                               None,\n",
        "                               vis_gt_coord[0],\n",
        "                               voxel_size=0.03,\n",
        "                               rotation_amount=np.deg2rad(rotation_amount))\n",
        "\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "plt.imshow(rendered_img)\n",
        "plt.axis('off')\n",
        "\n",
        "print(f\"Lang goal: {lang_goal}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 360 degree rendering:\n",
        "\n",
        "ff=[]\n",
        "for rotation_amount in range(-180,180,5):\n",
        " #rotation_amount = -180 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
        " rendered_img = visualise_voxel(vis_voxel_grid[0],\n",
        "                               None,\n",
        "                               None,\n",
        "                               vis_gt_coord[0],\n",
        "                               voxel_size=0.03,\n",
        "                               rotation_amount=np.deg2rad(rotation_amount))\n",
        "\n",
        " ff.append(rendered_img)"
      ],
      "metadata": {
        "id": "wGmCXiqPFVfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display.HTML(display_anim(ff))"
      ],
      "metadata": {
        "id": "Rvy8S8oDDT1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK7qhjkcSqhC"
      },
      "source": [
        "This visualization shows a voxel grid of size 100x100x100 = 1 million voxels, and one expert keyframe action (blue voxel). These samples are what PerAct is trained with. Given a language goal and voxel grid, we train a detector to detect the next best action with supervised learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeMSx_7pNouL"
      },
      "source": [
        "### PerceiverIO\n",
        "\n",
        "Now we can start implementing the actual Transformer backbone of PerAct.\n",
        "\n",
        "The input grid is 100×100×100 = 1 million voxels. If we extract 5×5×5 patches, the input is still 20×20×20 = 8000 embeddings long. This sequence is way too long for a standard Transformer with O(n^2) self-attention connections. So we use the [PerceiverIO architecture](https://arxiv.org/abs/2107.14795) instead.  \n",
        "\n",
        "Perceiver uses a small set of **latent vectors** to encode the input. These latent vectors are randomly initialized and trained end-to-end. This approach decouples the depth of the Transformer self-attention layers from the dimensionality of the input space, which allows us train PerAct on very large input voxel grids. We can potentially scale the input to 200^3 voxels without increasing self-attention layer parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afrMAUZ3pp8q"
      },
      "outputs": [],
      "source": [
        "# From: https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_io.py\n",
        "\n",
        "from math import pi, log\n",
        "from functools import wraps\n",
        "\n",
        "from einops import rearrange, repeat, reduce\n",
        "from einops.layers.torch import Reduce\n",
        "\n",
        "from arm.network_utils import Conv3DInceptionBlock, DenseBlock, SpatialSoftmax3D, Conv3DInceptionBlockUpsampleBlock, Conv3DBlock, Conv3DUpsampleBlock\n",
        "\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "\n",
        "def cache_fn(f):\n",
        "    cache = None\n",
        "\n",
        "    @wraps(f)\n",
        "    def cached_fn(*args, _cache=True, **kwargs):\n",
        "        if not _cache:\n",
        "            return f(*args, **kwargs)\n",
        "        nonlocal cache\n",
        "        if cache is not None:\n",
        "            return cache\n",
        "        cache = f(*args, **kwargs)\n",
        "        return cache\n",
        "\n",
        "    return cached_fn\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(self.norm_context):\n",
        "            context = kwargs['context']\n",
        "            normed_context = self.norm_context(context)\n",
        "            kwargs.update(context=normed_context)\n",
        "\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim=-1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module): # is all you need. Living up to its name. \n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        # attention\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        # dropout\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl9ZW-oCQA6l"
      },
      "source": [
        "We use these attention modules to construct PerceiverIO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rYuWNtRPEBy"
      },
      "outputs": [],
      "source": [
        "# PerceiverIO adapted for 6-DoF manipulation\n",
        "\n",
        "class PerceiverIO(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            depth,                    # number of self-attention layers\n",
        "            iterations,               # number cross-attention iterations (PerceiverIO uses just 1)\n",
        "            voxel_size,               # N voxels per side (size: N*N*N)\n",
        "            initial_dim,              # 10 dimensions - dimension of the input sequence to be encoded \n",
        "            low_dim_size,             # 4 dimensions - proprioception: {gripper_open, left_finger_joint, right_finger_joint, timestep}\n",
        "            layer=0,                  \n",
        "            num_rotation_classes=72,  # 5 degree increments (5*72=360) for each of the 3-axis\n",
        "            num_grip_classes=2,       # open or not open\n",
        "            num_collision_classes=2,  # collisions allowed or not allowed          \n",
        "            input_axis=3,             # 3D tensors have 3 axes\n",
        "            num_latents=512,          # number of latent vectors     \n",
        "            im_channels=64,           # intermediate channel size\n",
        "            latent_dim=512,           # dimensions of latent vectors\n",
        "            cross_heads=1,            # number of cross-attention heads\n",
        "            latent_heads=8,           # number of latent heads\n",
        "            cross_dim_head=64,        \n",
        "            latent_dim_head=64,\n",
        "            activation='relu',\n",
        "            weight_tie_layers=False,\n",
        "            input_dropout=0.1,\n",
        "            attn_dropout=0.1,\n",
        "            decoder_dropout=0.0,\n",
        "            voxel_patch_size=5,       # intial patch size\n",
        "            voxel_patch_stride=5,     # initial stride to patchify voxel input\n",
        "            final_dim=64,             # final dimensions of features\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.layer = layer\n",
        "        self.init_dim = int(initial_dim)\n",
        "        self.iterations = iterations\n",
        "        self.input_axis = input_axis\n",
        "        self.voxel_size = voxel_size\n",
        "        self.low_dim_size = low_dim_size\n",
        "        self.im_channels = im_channels\n",
        "        self.voxel_patch_size = voxel_patch_size\n",
        "        self.voxel_patch_stride = voxel_patch_stride\n",
        "        self.num_rotation_classes = num_rotation_classes\n",
        "        self.num_grip_classes = num_grip_classes\n",
        "        self.num_collision_classes = num_collision_classes\n",
        "        self.final_dim = final_dim\n",
        "        self.input_dropout = input_dropout\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.decoder_dropout = decoder_dropout\n",
        "\n",
        "        # patchified input dimensions\n",
        "        spatial_size = voxel_size // self.voxel_patch_stride # 100/5 = 20\n",
        "\n",
        "        # 64 voxel features + 64 proprio features\n",
        "        self.input_dim_before_seq = self.im_channels * 2\n",
        "\n",
        "        # learnable positional encoding\n",
        "        lang_emb_dim, lang_max_seq_len = 512, 77  \n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1,\n",
        "                                                     lang_max_seq_len+spatial_size**3,\n",
        "                                                     self.input_dim_before_seq))\n",
        "\n",
        "        # voxel input preprocessing encoder\n",
        "        self.input_preprocess = Conv3DBlock(\n",
        "            self.init_dim, self.im_channels, kernel_sizes=1, strides=1,\n",
        "            norm=None, activation=activation,\n",
        "        )\n",
        "\n",
        "        # proprio preprocessing encoder\n",
        "        self.proprio_preprocess = DenseBlock(\n",
        "            self.low_dim_size, self.im_channels, norm=None, activation=activation,\n",
        "        )\n",
        "        \n",
        "        # patchify conv\n",
        "        self.patchify = Conv3DBlock(\n",
        "            self.input_preprocess.out_channels, self.im_channels,\n",
        "            kernel_sizes=self.voxel_patch_size, strides=self.voxel_patch_stride,\n",
        "            norm=None, activation=activation)\n",
        "\n",
        "        # lang preprocess\n",
        "        self.lang_preprocess = nn.Linear(lang_emb_dim, self.im_channels * 2)\n",
        "\n",
        "        # pooling functions\n",
        "        self.local_maxp = nn.MaxPool3d(3, 2, padding=1)\n",
        "        self.global_maxp = nn.AdaptiveMaxPool3d(1)\n",
        "\n",
        "        # 1st 3D softmax\n",
        "        self.ss0 = SpatialSoftmax3D(\n",
        "            self.voxel_size, self.voxel_size, self.voxel_size,\n",
        "            self.im_channels)\n",
        "        flat_size = self.im_channels * 4\n",
        "\n",
        "        # latent vectors (that are randomly initialized)\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "\n",
        "        # encoder cross attention\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, self.input_dim_before_seq, heads=cross_heads,\n",
        "                                          dim_head=cross_dim_head, dropout=input_dropout),\n",
        "                    context_dim=self.input_dim_before_seq),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim,\n",
        "                                          Attention(latent_dim, heads=latent_heads,\n",
        "                                                    dim_head=latent_dim_head, dropout=attn_dropout))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "\n",
        "        # self-attention layers\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                get_latent_attn(**cache_args),\n",
        "                get_latent_ff(**cache_args)\n",
        "            ]))\n",
        "\n",
        "        # decoder cross attention\n",
        "        self.decoder_cross_attn = PreNorm(self.input_dim_before_seq, Attention(self.input_dim_before_seq, latent_dim, heads=cross_heads,\n",
        "                                                                      dim_head=cross_dim_head,\n",
        "                                                                      dropout=decoder_dropout),\n",
        "                                          context_dim=latent_dim)\n",
        "\n",
        "        # upsample conv\n",
        "        self.up0 = Conv3DUpsampleBlock(\n",
        "            self.input_dim_before_seq, self.final_dim,\n",
        "            kernel_sizes=self.voxel_patch_size, strides=self.voxel_patch_stride,\n",
        "            norm=None, activation=activation,\n",
        "        )\n",
        "\n",
        "        # 2nd 3D softmax\n",
        "        self.ss1 = SpatialSoftmax3D(\n",
        "            spatial_size, spatial_size, spatial_size,\n",
        "            self.input_dim_before_seq)\n",
        "\n",
        "        flat_size += self.input_dim_before_seq * 4\n",
        "\n",
        "        # final layers\n",
        "        self.final = Conv3DBlock(\n",
        "            self.im_channels * 2,\n",
        "            self.im_channels,\n",
        "            kernel_sizes=3,\n",
        "            strides=1, norm=None, activation=activation)\n",
        "\n",
        "        # 100x100x100x64 -> 100x100x100x1 decoder for translation Q-values\n",
        "        self.trans_decoder = Conv3DBlock(\n",
        "            self.final_dim, 1, kernel_sizes=3, strides=1,\n",
        "            norm=None, activation=None,\n",
        "        )\n",
        "\n",
        "        # final 3D softmax\n",
        "        self.ss_final = SpatialSoftmax3D(\n",
        "            self.voxel_size, self.voxel_size, self.voxel_size,\n",
        "            self.im_channels)\n",
        "\n",
        "        flat_size += self.im_channels * 4\n",
        "\n",
        "        # MLP layers\n",
        "        self.dense0 =  DenseBlock(\n",
        "            flat_size, 256, None, activation)\n",
        "        self.dense1 = DenseBlock(\n",
        "            256, self.final_dim, None, activation)\n",
        "\n",
        "        # 1x64 -> 1x(72+72+72+2+2) decoders for rotation, gripper open, and collision Q-values\n",
        "        self.rot_grip_collision_ff = DenseBlock(self.final_dim,\n",
        "                                          self.num_rotation_classes * 3 + \\\n",
        "                                          self.num_grip_classes + \\\n",
        "                                          self.num_collision_classes,\n",
        "                                          None, None)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            ins,\n",
        "            proprio,\n",
        "            lang_goal_embs,\n",
        "            bounds,\n",
        "            mask=None,\n",
        "    ):\n",
        "        # preprocess\n",
        "        d0 = self.input_preprocess(ins)               # [B,10,100,100,100] -> [B,64,100,100,100]\n",
        "\n",
        "        # aggregated features from 1st softmax and maxpool for MLP decoders\n",
        "        feats = [self.ss0(d0.contiguous()), self.global_maxp(d0).view(ins.shape[0], -1)]\n",
        "        \n",
        "        # patchify input (5x5x5 patches)\n",
        "        ins = self.patchify(d0)                       # [B,64,100,100,100] -> [B,64,20,20,20]\n",
        "\n",
        "        b, c, d, h, w, device = *ins.shape, ins.device\n",
        "        axis = [d, h, w]\n",
        "        assert len(axis) == self.input_axis, 'input must have the same number of axis as input_axis'\n",
        "\n",
        "        # concat proprio\n",
        "        p = self.proprio_preprocess(proprio)          # [B,4] -> [B,64]\n",
        "        p = p.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, d, h, w)\n",
        "        ins = torch.cat([ins, p], dim=1)              # [B,128,20,20,20]\n",
        "\n",
        "        # channel last\n",
        "        ins = rearrange(ins, 'b d ... -> b ... d')    # [B,20,20,20,128]\n",
        "\n",
        "        # save original shape of input for layer\n",
        "        ins_orig_shape = ins.shape\n",
        "\n",
        "        # flatten voxel grid into sequence\n",
        "        ins = rearrange(ins, 'b ... d -> b (...) d')  # [B,8000,128]\n",
        "\n",
        "        # append language features as sequence\n",
        "        l = self.lang_preprocess(lang_goal_embs)      # [B,77,1024] -> [B,77,128]\n",
        "        ins = torch.cat((l, ins), dim=1)              # [B,8077,128]\n",
        "\n",
        "        # add learable pos encoding\n",
        "        ins = ins + self.pos_encoding\n",
        "\n",
        "        # batchify latents\n",
        "        x = repeat(self.latents, 'n d -> b n d', b=b)\n",
        "\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "\n",
        "        for it in range(self.iterations):\n",
        "            # encoder cross attention\n",
        "            x = cross_attn(x, context=ins, mask=mask) + x\n",
        "            x = cross_ff(x) + x\n",
        "\n",
        "            # self-attention layers\n",
        "            for self_attn, self_ff in self.layers:\n",
        "                x = self_attn(x) + x\n",
        "                x = self_ff(x) + x\n",
        "\n",
        "        # decoder cross attention\n",
        "        latents = self.decoder_cross_attn(ins, context=x)\n",
        "        latents = latents[:, l.shape[1]:]\n",
        "\n",
        "        # reshape back to voxel grid\n",
        "        latents = latents.view(b, *ins_orig_shape[1:-1], latents.shape[-1])  # [B,20,20,20,64]\n",
        "        latents = rearrange(latents, 'b ... d -> b d ...')                   # [B,64,20,20,20]\n",
        "\n",
        "        # aggregated features from 2nd softmax and maxpool for MLP decoders\n",
        "        feats.extend([self.ss1(latents.contiguous()), self.global_maxp(latents).view(b, -1)])\n",
        "\n",
        "        # upsample layer\n",
        "        u0 = self.up0(latents)                         # [B,64,100,100,100]\n",
        "\n",
        "        # skip connection like in UNets\n",
        "        u = self.final(torch.cat([d0, u0], dim=1))     # [B,64+64,100,100,100] -> [B,64,100,100,100]\n",
        "\n",
        "        # translation decoder\n",
        "        trans = self.trans_decoder(u)                  # [B,64,100,100,100] -> [B,1,100,100,100]\n",
        "        \n",
        "        # aggregated features from final softmax and maxpool for MLP decoders\n",
        "        feats.extend([self.ss_final(u.contiguous()), self.global_maxp(u).view(b, -1)])\n",
        "\n",
        "        # decoder MLP layers for rotation, gripper open, and collision\n",
        "        dense0 = self.dense0(torch.cat(feats, dim=1))\n",
        "        dense1 = self.dense1(dense0)                   # [B,72*3+2+2]\n",
        "        \n",
        "        # format output\n",
        "        rot_and_grip_collision_out = self.rot_grip_collision_ff(dense1)\n",
        "        rot_and_grip_out = rot_and_grip_collision_out[:, :-self.num_collision_classes]\n",
        "        collision_out = rot_and_grip_collision_out[:, -self.num_collision_classes:]\n",
        "\n",
        "        return trans, rot_and_grip_out, collision_out\n",
        "\n",
        "\n",
        "# initialize PerceiverIO Transformer\n",
        "perceiver_encoder = PerceiverIO(\n",
        "    depth=6,                     \n",
        "    iterations=1,                \n",
        "    voxel_size=VOXEL_SIZES[0],   \n",
        "    initial_dim=3 + 3 + 1 + 3,   \n",
        "    low_dim_size=4,              \n",
        "    layer=0,\n",
        "    num_rotation_classes=72,     \n",
        "    num_grip_classes=2,          \n",
        "    num_collision_classes=2,     \n",
        "    num_latents=NUM_LATENTS,     \n",
        "    latent_dim=512,              \n",
        "    cross_heads=1,   \n",
        "    latent_heads=8,\n",
        "    cross_dim_head=64,\n",
        "    latent_dim_head=64,\n",
        "    weight_tie_layers=False,   \n",
        "    activation='lrelu',\n",
        "    input_dropout=0.1,\n",
        "    attn_dropout=0.1,\n",
        "    decoder_dropout=0.0,\n",
        "    voxel_patch_size=5,           \n",
        "    voxel_patch_stride=5,        \n",
        "    final_dim=64,                \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbN4-D4ySSaK"
      },
      "source": [
        "### Q Functions\n",
        "\n",
        "Finally we put everything together to make PerAct's Q-Functions.  \n",
        "\n",
        "This module voxelizes RGB-D input, encodes per-voxel features, and predicts discretized actions.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQH8qnWG7caH"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class QFunction(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 perceiver_encoder: nn.Module,\n",
        "                 voxel_grid: VoxelGrid,\n",
        "                 rotation_resolution: float,\n",
        "                 device,\n",
        "                 training):\n",
        "        super(QFunction, self).__init__()\n",
        "        self._rotation_resolution = rotation_resolution\n",
        "        self._voxel_grid = voxel_grid\n",
        "        self._qnet = copy.deepcopy(perceiver_encoder)\n",
        "        self._qnet._dev = device\n",
        "\n",
        "    def _argmax_3d(self, tensor_orig):\n",
        "        b, c, d, h, w = tensor_orig.shape  # c will be one\n",
        "        idxs = tensor_orig.view(b, c, -1).argmax(-1)\n",
        "        indices = torch.cat([((idxs // h) // d), (idxs // h) % w, idxs % w], 1)\n",
        "        return indices\n",
        "\n",
        "    def choose_highest_action(self, q_trans, q_rot_grip, q_collision):\n",
        "        coords = self._argmax_3d(q_trans)\n",
        "        rot_and_grip_indicies = None\n",
        "        if q_rot_grip is not None:\n",
        "            q_rot = torch.stack(torch.split(\n",
        "                q_rot_grip[:, :-2],\n",
        "                int(360 // self._rotation_resolution),\n",
        "                dim=1), dim=1)\n",
        "            rot_and_grip_indicies = torch.cat(\n",
        "                [q_rot[:, 0:1].argmax(-1),\n",
        "                 q_rot[:, 1:2].argmax(-1),\n",
        "                 q_rot[:, 2:3].argmax(-1),\n",
        "                 q_rot_grip[:, -2:].argmax(-1, keepdim=True)], -1)\n",
        "            ignore_collision = q_collision[:, -2:].argmax(-1, keepdim=True)\n",
        "        return coords, rot_and_grip_indicies, ignore_collision\n",
        "\n",
        "    def forward(self, \n",
        "                obs, \n",
        "                proprio, \n",
        "                pcd, \n",
        "                lang_goal_embs,\n",
        "                bounds=None):\n",
        "\n",
        "        # flatten point cloud\n",
        "        bs = obs[0][0].shape[0]\n",
        "        pcd_flat = torch.cat(\n",
        "            [p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in pcd], 1)\n",
        "\n",
        "        # flatten rgb\n",
        "        image_features = [o[0] for o in obs]\n",
        "        feat_size = image_features[0].shape[1]\n",
        "        flat_imag_features = torch.cat(\n",
        "            [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in\n",
        "             image_features], 1)\n",
        "\n",
        "        # voxelize\n",
        "        voxel_grid = self._voxel_grid.coords_to_bounding_voxel_grid(\n",
        "            pcd_flat, coord_features=flat_imag_features, coord_bounds=bounds)\n",
        "\n",
        "        # swap to channels fist\n",
        "        voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach()\n",
        "\n",
        "        # batch bounds if necessary\n",
        "        if bounds.shape[0] != bs:\n",
        "            bounds = bounds.repeat(bs, 1)\n",
        "\n",
        "        # forward pass\n",
        "        q_trans, rot_and_grip_q, collision_q = self._qnet(voxel_grid, \n",
        "                                                          proprio, \n",
        "                                                          lang_goal_embs,\n",
        "                                                          bounds)\n",
        "        return q_trans, rot_and_grip_q, collision_q, voxel_grid\n",
        "\n",
        "    def latents(self):\n",
        "        return self._qnet.latent_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70Fs_PQz0Cx"
      },
      "source": [
        "### PerAct Agent\n",
        "\n",
        "Let's initialize PerAct and define an update function for the training loop. \n",
        "\n",
        "The keyframe actions used for supervision are represented as one-hot vectors. Then we use cross-entropy loss to train PerAct, just like a standard classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8BqH-grZvoZ"
      },
      "outputs": [],
      "source": [
        "from arm.optim.lamb import Lamb\n",
        "from arm.utils import stack_on_channel\n",
        "\n",
        "\n",
        "class PerceiverActorAgent():\n",
        "    def __init__(self,\n",
        "                coordinate_bounds: list,\n",
        "                perceiver_encoder: nn.Module,\n",
        "                camera_names: list,\n",
        "                batch_size: int,\n",
        "                voxel_size: int,\n",
        "                voxel_feature_size: int,\n",
        "                num_rotation_classes: int,\n",
        "                rotation_resolution: float,\n",
        "                lr: float = 0.0001,\n",
        "                image_resolution: list = None,\n",
        "                lambda_weight_l2: float = 0.0,\n",
        "                transform_augmentation: bool = True,\n",
        "                transform_augmentation_xyz: list = [0.0, 0.0, 0.0],\n",
        "                transform_augmentation_rpy: list = [0.0, 0.0, 180.0],\n",
        "                transform_augmentation_rot_resolution: int = 5,\n",
        "                optimizer_type: str = 'lamb'):\n",
        "\n",
        "        self._coordinate_bounds = coordinate_bounds\n",
        "        self._perceiver_encoder = perceiver_encoder\n",
        "        self._camera_names = camera_names\n",
        "        self._batch_size = batch_size\n",
        "        self._voxel_size = voxel_size\n",
        "        self._voxel_feature_size = voxel_feature_size\n",
        "        self._num_rotation_classes = num_rotation_classes\n",
        "        self._rotation_resolution = rotation_resolution\n",
        "        self._lr = lr\n",
        "        self._image_resolution = image_resolution\n",
        "        self._lambda_weight_l2 = lambda_weight_l2\n",
        "        self._transform_augmentation = transform_augmentation\n",
        "        self._transform_augmentation_xyz = transform_augmentation_xyz\n",
        "        self._transform_augmentation_rpy = transform_augmentation_rpy\n",
        "        self._transform_augmentation_rot_resolution = transform_augmentation_rot_resolution\n",
        "        self._optimizer_type = optimizer_type\n",
        "\n",
        "        self._cross_entropy_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def build(self, training: bool, device: torch.device = None):\n",
        "        self._training = training\n",
        "        self._device = device\n",
        "\n",
        "        vox_grid = VoxelGrid(\n",
        "            coord_bounds=self._coordinate_bounds,\n",
        "            voxel_size=self._voxel_size,\n",
        "            device=device,\n",
        "            batch_size=self._batch_size,\n",
        "            feature_size=self._voxel_feature_size,\n",
        "            max_num_coords=np.prod([IMAGE_SIZE, IMAGE_SIZE]) * len(CAMERAS),\n",
        "        )\n",
        "        self._vox_grid = vox_grid\n",
        "\n",
        "        self._q = QFunction(self._perceiver_encoder,\n",
        "                            vox_grid,\n",
        "                            self._rotation_resolution,\n",
        "                            device,\n",
        "                            training).to(device).train(training)\n",
        "\n",
        "        self._coordinate_bounds = torch.tensor(self._coordinate_bounds,\n",
        "                                               device=device).unsqueeze(0)\n",
        "\n",
        "        if self._optimizer_type == 'lamb':\n",
        "            # From: https://github.com/cybertronai/pytorch-lamb/blob/master/pytorch_lamb/lamb.py\n",
        "            self._optimizer = Lamb(\n",
        "                self._q.parameters(),\n",
        "                lr=self._lr,\n",
        "                weight_decay=self._lambda_weight_l2,\n",
        "                betas=(0.9, 0.999),\n",
        "                adam=False,\n",
        "            )\n",
        "        elif self._optimizer_type == 'adam':\n",
        "            self._optimizer = torch.optim.Adam(\n",
        "                self._q.parameters(),\n",
        "                lr=self._lr,\n",
        "                weight_decay=self._lambda_weight_l2,\n",
        "            )\n",
        "        else:\n",
        "            raise Exception('Unknown optimizer')\n",
        "\n",
        "    def _softmax_q(self, q):\n",
        "        q_shape = q.shape\n",
        "        return F.softmax(q.reshape(q_shape[0], -1), dim=1).reshape(q_shape)\n",
        "    \n",
        "    def _get_one_hot_expert_actions(self,  # You don't really need this function since GT labels are already in the right format. \n",
        "                                    #This is some leftover code from my experiments with label smoothing.\n",
        "                                    batch_size,\n",
        "                                    action_trans,\n",
        "                                    action_rot_grip,\n",
        "                                    action_ignore_collisions,\n",
        "                                    device):\n",
        "        bs = batch_size\n",
        "\n",
        "        # initialize with zero tensors\n",
        "        action_trans_one_hot = torch.zeros((bs, self._voxel_size, self._voxel_size, self._voxel_size), dtype=int, device=device)\n",
        "        action_rot_x_one_hot = torch.zeros((bs, self._num_rotation_classes), dtype=int, device=device)\n",
        "        action_rot_y_one_hot = torch.zeros((bs, self._num_rotation_classes), dtype=int, device=device)\n",
        "        action_rot_z_one_hot = torch.zeros((bs, self._num_rotation_classes), dtype=int, device=device)\n",
        "        action_grip_one_hot  = torch.zeros((bs, 2), dtype=int, device=device)\n",
        "        action_collision_one_hot = torch.zeros((bs, 2), dtype=int, device=device) \n",
        "\n",
        "        # fill one-hots\n",
        "        for b in range(bs):\n",
        "          # translation\n",
        "          gt_coord = action_trans[b, :]\n",
        "          action_trans_one_hot[b, gt_coord[0], gt_coord[1], gt_coord[2]] = 1\n",
        "\n",
        "          # rotation\n",
        "          gt_rot_grip = action_rot_grip[b, :]\n",
        "          action_rot_x_one_hot[b, gt_rot_grip[0]] = 1\n",
        "          action_rot_y_one_hot[b, gt_rot_grip[1]] = 1\n",
        "          action_rot_z_one_hot[b, gt_rot_grip[2]] = 1\n",
        "          action_grip_one_hot[b, gt_rot_grip[3]] = 1\n",
        "\n",
        "          # ignore collision\n",
        "          gt_ignore_collisions = action_ignore_collisions[b, :]\n",
        "          action_collision_one_hot[b, gt_ignore_collisions[0]] = 1\n",
        "        \n",
        "        # flatten trans\n",
        "        action_trans_one_hot = action_trans_one_hot.view(bs, -1) \n",
        "\n",
        "        return action_trans_one_hot, \\\n",
        "               action_rot_x_one_hot, \\\n",
        "               action_rot_y_one_hot, \\\n",
        "               action_rot_z_one_hot, \\\n",
        "               action_grip_one_hot,  \\\n",
        "               action_collision_one_hot\n",
        "\n",
        "\n",
        "    def update(self, step: int, replay_sample: dict, backprop: bool = True) -> dict:\n",
        "        # sample\n",
        "        action_trans = replay_sample['trans_action_indicies'][:, -1, :3].int()\n",
        "        action_rot_grip = replay_sample['rot_grip_action_indicies'][:, -1].int()\n",
        "        action_ignore_collisions = replay_sample['ignore_collisions'][:, -1].int()\n",
        "        action_gripper_pose = replay_sample['gripper_pose'][:, -1]\n",
        "        lang_goal_embs = replay_sample['lang_goal_embs'][:, -1].float()\n",
        "        \n",
        "        # metric scene bounds\n",
        "        bounds = bounds_tp1 = self._coordinate_bounds\n",
        "\n",
        "        # inputs\n",
        "        proprio = stack_on_channel(replay_sample['low_dim_state'])\n",
        "        obs, pcd = _preprocess_inputs(replay_sample)\n",
        "\n",
        "        # TODO: data augmentation by applying SE(3) pertubations to pcd and actions\n",
        "        # see https://github.com/peract/peract/blob/main/voxel/augmentation.py#L68 for reference\n",
        "\n",
        "        # Q function\n",
        "        q_trans, rot_grip_q, collision_q, voxel_grid = self._q(obs,\n",
        "                                                               proprio,\n",
        "                                                               pcd,\n",
        "                                                               lang_goal_embs,\n",
        "                                                               bounds)\n",
        "        \n",
        "        # one-hot expert actions\n",
        "        bs = self._batch_size\n",
        "        action_trans_one_hot, action_rot_x_one_hot, \\\n",
        "        action_rot_y_one_hot, action_rot_z_one_hot, \\\n",
        "        action_grip_one_hot, action_collision_one_hot = self._get_one_hot_expert_actions(bs,\n",
        "                                                                                         action_trans,\n",
        "                                                                                         action_rot_grip,\n",
        "                                                                                         action_ignore_collisions,\n",
        "                                                                                         device=self._device)\n",
        "        total_loss = 0.\n",
        "        if backprop:\n",
        "            # cross-entropy loss\n",
        "            trans_loss = self._cross_entropy_loss(q_trans.view(bs, -1), \n",
        "                                                  action_trans_one_hot.argmax(-1))\n",
        "            \n",
        "            rot_grip_loss = 0.\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 0*self._num_rotation_classes:1*self._num_rotation_classes], \n",
        "                                                      action_rot_x_one_hot.argmax(-1))\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 1*self._num_rotation_classes:2*self._num_rotation_classes], \n",
        "                                                      action_rot_y_one_hot.argmax(-1))\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 2*self._num_rotation_classes:3*self._num_rotation_classes], \n",
        "                                                      action_rot_z_one_hot.argmax(-1))\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 3*self._num_rotation_classes:],\n",
        "                                                      action_grip_one_hot.argmax(-1))\n",
        "            \n",
        "            collision_loss = self._cross_entropy_loss(collision_q,\n",
        "                                                      action_collision_one_hot.argmax(-1))\n",
        "            \n",
        "            total_loss = trans_loss + rot_grip_loss + collision_loss\n",
        "            total_loss = total_loss.mean()\n",
        "\n",
        "            # backprop\n",
        "            self._optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            self._optimizer.step()\n",
        "\n",
        "            total_loss = total_loss.item()\n",
        "\n",
        "        # choose best action through argmax\n",
        "        coords_indicies, rot_and_grip_indicies, ignore_collision_indicies = self._q.choose_highest_action(q_trans,\n",
        "                                                                                                          rot_grip_q,\n",
        "                                                                                                          collision_q)\n",
        "        \n",
        "        # discrete to continuous translation action\n",
        "        res = (bounds[:, 3:] - bounds[:, :3]) / self._voxel_size\n",
        "        continuous_trans = bounds[:, :3] + res * coords_indicies.int() + res / 2\n",
        "        \n",
        "        return {\n",
        "            'total_loss': total_loss,\n",
        "            'voxel_grid': voxel_grid,\n",
        "            'q_trans': self._softmax_q(q_trans),\n",
        "            'pred_action': {\n",
        "                'trans': coords_indicies,\n",
        "                'continuous_trans': continuous_trans,\n",
        "                'rot_and_grip': rot_and_grip_indicies,\n",
        "                'collision': ignore_collision_indicies\n",
        "            },\n",
        "            'expert_action': {\n",
        "                'action_trans': action_trans\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def compute_loss(self, step: int, replay_sample: dict):\n",
        "        action_trans = replay_sample['trans_action_indicies'][:, -1, :3].int()\n",
        "        action_rot_grip = replay_sample['rot_grip_action_indicies'][:, -1].int()\n",
        "        action_ignore_collisions = replay_sample['ignore_collisions'][:, -1].int()\n",
        "        action_gripper_pose = replay_sample['gripper_pose'][:, -1]\n",
        "        lang_goal_embs = replay_sample['lang_goal_embs'][:, -1].float()\n",
        "        \n",
        "        bounds = bounds_tp1 = self._coordinate_bounds\n",
        "        proprio = stack_on_channel(replay_sample['low_dim_state'])\n",
        "        obs, pcd = _preprocess_inputs(replay_sample)\n",
        "\n",
        "        q_trans, rot_grip_q, collision_q, voxel_grid = self._q(obs,proprio,pcd,lang_goal_embs,bounds)\n",
        "        \n",
        "        bs = self._batch_size\n",
        "        action_trans_one_hot, action_rot_x_one_hot, \\\n",
        "        action_rot_y_one_hot, action_rot_z_one_hot, \\\n",
        "        action_grip_one_hot, action_collision_one_hot = \\\n",
        "        self._get_one_hot_expert_actions(bs,action_trans,action_rot_grip,action_ignore_collisions,device=self._device)\n",
        "        total_loss = 0.\n",
        "\n",
        "        trans_loss = self._cross_entropy_loss(q_trans.view(bs, -1), action_trans_one_hot.argmax(-1))\n",
        "        \n",
        "        rot_grip_loss = 0.\n",
        "        rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 0*self._num_rotation_classes:1*self._num_rotation_classes], \n",
        "                                                  action_rot_x_one_hot.argmax(-1))\n",
        "        rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 1*self._num_rotation_classes:2*self._num_rotation_classes], \n",
        "                                                  action_rot_y_one_hot.argmax(-1))\n",
        "        rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 2*self._num_rotation_classes:3*self._num_rotation_classes], \n",
        "                                                  action_rot_z_one_hot.argmax(-1))\n",
        "        rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 3*self._num_rotation_classes:],\n",
        "                                                  action_grip_one_hot.argmax(-1))\n",
        "        \n",
        "        collision_loss = self._cross_entropy_loss(collision_q,\n",
        "                                                  action_collision_one_hot.argmax(-1))\n",
        "        \n",
        "        total_loss = trans_loss + rot_grip_loss + collision_loss\n",
        "        total_loss = total_loss.mean()\n",
        "        total_loss = total_loss.item()\n",
        "        return total_loss\n",
        "\n",
        "# initialize PerceiverActor\n",
        "peract_agent = PerceiverActorAgent(\n",
        "    coordinate_bounds=SCENE_BOUNDS,\n",
        "    perceiver_encoder=perceiver_encoder,\n",
        "    camera_names=CAMERAS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    voxel_size=VOXEL_SIZES[0],\n",
        "    voxel_feature_size=3,\n",
        "    num_rotation_classes=72,\n",
        "    rotation_resolution=5,\n",
        "    lr=0.0001,\n",
        "    image_resolution=[IMAGE_SIZE, IMAGE_SIZE],\n",
        "    lambda_weight_l2=0.000001,\n",
        "    transform_augmentation=False,\n",
        "    optimizer_type='lamb',\n",
        ")\n",
        "peract_agent.build(training=True, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pre-trained weights\n",
        "\n",
        "If you want you can load pretrained weights (on a different data set). This model was pretrained on pick and place for approximately 1hr."
      ],
      "metadata": {
        "id": "cZrQ84kZGiVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 12flSBw6m3hqzxMP5G77sah0N_luWFiqa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1SS4evJGo97",
        "outputId": "e565b4bc-3dee-43ef-cc13-bf11541e80ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12flSBw6m3hqzxMP5G77sah0N_luWFiqa\n",
            "To: /content/model.pth\n",
            "100% 154M/154M [00:04<00:00, 34.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load:\n",
        "saved_model_PATH = 'model.pth'\n",
        "\n",
        "peract_agent._q._qnet.load_state_dict(torch.load(saved_model_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RetOUFhaGhAE",
        "outputId": "531f1e5d-4e91-4e3d-ce85-a545a5493856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG1IZxjsz7Om"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "The final training loop samples data from the replay buffer and trains the agent with supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC-6HBZOv0PW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "LOG_FREQ = 50\n",
        "TRAINING_ITERATIONS = 5*2400\n",
        "\n",
        "start_time = time.time()\n",
        "for iteration in range(TRAINING_ITERATIONS):\n",
        "    batch = next(train_data_iter)\n",
        "    batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
        "    update_dict = peract_agent.update(iteration, batch)\n",
        "\n",
        "    if iteration % LOG_FREQ == 0:\n",
        "      elapsed_time = (time.time() - start_time) / 60.0\n",
        "      print(\"Total Loss: %f | Elapsed Time: %f mins\" % (update_dict['total_loss'], elapsed_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Test loss"
      ],
      "metadata": {
        "id": "iGCN_GId094o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(peract, step, replay_sample):\n",
        "    action_trans = replay_sample['trans_action_indicies'][:, -1, :3].int()\n",
        "    action_rot_grip = replay_sample['rot_grip_action_indicies'][:, -1].int()\n",
        "    action_ignore_collisions = replay_sample['ignore_collisions'][:, -1].int()\n",
        "    action_gripper_pose = replay_sample['gripper_pose'][:, -1]\n",
        "    lang_goal_embs = replay_sample['lang_goal_embs'][:, -1].float()\n",
        "\n",
        "    bounds = bounds_tp1 = peract._coordinate_bounds\n",
        "    proprio = stack_on_channel(replay_sample['low_dim_state'])\n",
        "    obs, pcd = _preprocess_inputs(replay_sample)\n",
        "    \n",
        "    q_trans, rot_grip_q, collision_q, voxel_grid = peract._q(obs,proprio,pcd,lang_goal_embs,bounds)\n",
        "    \n",
        "    bs = peract._batch_size\n",
        "    action_trans_one_hot, action_rot_x_one_hot, \\\n",
        "    action_rot_y_one_hot, action_rot_z_one_hot, \\\n",
        "    action_grip_one_hot, action_collision_one_hot \\\n",
        "     = peract._get_one_hot_expert_actions(bs,action_trans,action_rot_grip,action_ignore_collisions,device=peract._device)\n",
        "    \n",
        "    total_loss = 0.\n",
        "    trans_loss = peract._cross_entropy_loss(q_trans.view(bs, -1), \n",
        "                                          action_trans_one_hot.argmax(-1))\n",
        "    \n",
        "    rot_grip_loss = 0.\n",
        "    rot_grip_loss += peract._cross_entropy_loss(rot_grip_q[:, 0*peract._num_rotation_classes:1*peract._num_rotation_classes], \n",
        "                                              action_rot_x_one_hot.argmax(-1))\n",
        "    rot_grip_loss += peract._cross_entropy_loss(rot_grip_q[:, 1*peract._num_rotation_classes:2*peract._num_rotation_classes], \n",
        "                                              action_rot_y_one_hot.argmax(-1))\n",
        "    rot_grip_loss += peract._cross_entropy_loss(rot_grip_q[:, 2*peract._num_rotation_classes:3*peract._num_rotation_classes], \n",
        "                                              action_rot_z_one_hot.argmax(-1))\n",
        "    rot_grip_loss += peract._cross_entropy_loss(rot_grip_q[:, 3*peract._num_rotation_classes:],\n",
        "                                              action_grip_one_hot.argmax(-1))\n",
        "    \n",
        "    collision_loss = peract._cross_entropy_loss(collision_q,action_collision_one_hot.argmax(-1))\n",
        "    \n",
        "    total_loss = trans_loss + rot_grip_loss + collision_loss\n",
        "    total_loss = total_loss.mean()\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "Ip0Hh3_Apj6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    batch = next(train_data_iter)\n",
        "    batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
        "    print(compute_loss(peract_agent,iteration, batch))"
      ],
      "metadata": {
        "id": "wUFZWr76z2vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save model"
      ],
      "metadata": {
        "id": "XO-caqOP1Y-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save and load:\n",
        "save_model_PATH = 'model_.pth'\n",
        "\n",
        "torch.save(peract_agent._q._qnet.state_dict(), model_PATH)\n",
        "\n",
        "#model = TheModelClass(*args, **kwargs)\n",
        "#peract_agent._q._qnet.load_state_dict(torch.load('gdrive/MyDrive/Robot/data/model.pth'))\n",
        "#model.eval()"
      ],
      "metadata": {
        "id": "thZuq5W3inWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpJPZzfcUV96"
      },
      "source": [
        "### Inference and Visualization\n",
        "\n",
        "Let's see how PerAct does on held-out test data.  \n",
        "\n",
        "PerAct should be evaluated in simulation on scenes with randomized object poses and object instances. But this Colab notebook doesn't support the V-REP simulator (for RLBench tasks). So for now we will do inference on a static test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehbeZdHdEwVC"
      },
      "outputs": [],
      "source": [
        "from arm.utils import visualise_voxel, discrete_euler_to_quaternion, get_gripper_render_pose\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "batch = next(test_data_iter)\n",
        "lang_goal = batch['lang_goal'][0][0][0]\n",
        "batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
        "update_dict = peract_agent.update(iteration, batch, backprop=False)\n",
        "\n",
        "# things to visualize\n",
        "vis_voxel_grid = update_dict['voxel_grid'][0].detach().cpu().numpy()\n",
        "vis_trans_q = update_dict['q_trans'][0].detach().cpu().numpy()\n",
        "vis_trans_coord = update_dict['pred_action']['trans'][0].detach().cpu().numpy()\n",
        "vis_gt_coord = update_dict['expert_action']['action_trans'][0].detach().cpu().numpy()\n",
        "\n",
        "# discrete to continuous\n",
        "continuous_trans = update_dict['pred_action']['continuous_trans'][0].detach().cpu().numpy()\n",
        "continuous_quat = discrete_euler_to_quaternion(update_dict['pred_action']['rot_and_grip'][0][:3].detach().cpu().numpy(),\n",
        "                                               resolution=peract_agent._rotation_resolution)\n",
        "gripper_open = bool(update_dict['pred_action']['rot_and_grip'][0][-1].detach().cpu().numpy())\n",
        "ignore_collision = bool(update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
        "\n",
        "# gripper visualization pose\n",
        "voxel_size = 0.045\n",
        "voxel_scale = voxel_size * 100\n",
        "gripper_pose_mat = get_gripper_render_pose(voxel_scale, \n",
        "                                           SCENE_BOUNDS[:3],\n",
        "                                           continuous_trans,\n",
        "                                           continuous_quat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL-RyskRiA_u"
      },
      "outputs": [],
      "source": [
        "#@markdown #### Show Q-Prediction and Best Action\n",
        "show_expert_action = True  #@param {type:\"boolean\"}\n",
        "show_q_values = True  #@param {type:\"boolean\"}\n",
        "render_gripper = True  #@param {type:\"boolean\"}\n",
        "rotation_amount = -125 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
        "\n",
        "rendered_img = visualise_voxel(vis_voxel_grid,\n",
        "                               vis_trans_q if show_q_values else None,\n",
        "                               vis_trans_coord,\n",
        "                               vis_gt_coord if show_expert_action else None,\n",
        "                               voxel_size=voxel_size,\n",
        "                               rotation_amount=np.deg2rad(rotation_amount),\n",
        "                               render_gripper=render_gripper,\n",
        "                               gripper_pose=gripper_pose_mat,\n",
        "                               gripper_mesh_scale=voxel_scale)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "plt.imshow(rendered_img)\n",
        "plt.axis('off')\n",
        "\n",
        "print(f\"Lang Goal: {lang_goal}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 360 degree rendering:\n",
        "ff=[]\n",
        "for rotation_amount in range(-180,180,5):\n",
        "  #rotation_amount = -180 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
        "  #rotation_amount = 0 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
        "\n",
        "  rendered_img = visualise_voxel(vis_voxel_grid,\n",
        "                                vis_trans_q if show_q_values else None,\n",
        "                                vis_trans_coord,\n",
        "                                vis_gt_coord if show_expert_action else None,\n",
        "                                voxel_size=voxel_size,\n",
        "                                rotation_amount=np.deg2rad(rotation_amount),\n",
        "                                render_gripper=render_gripper,\n",
        "                                gripper_pose=gripper_pose_mat,\n",
        "                                gripper_mesh_scale=voxel_scale)\n",
        "\n",
        "  ff.append(rendered_img)"
      ],
      "metadata": {
        "id": "xSW_9Y8fVnpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " display.HTML(display_anim(ff))"
      ],
      "metadata": {
        "id": "e4Mcii8BVnpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUM917haS8MZ"
      },
      "source": [
        ":The red values are normalized translation Q-predictions (they might be barely visible if the distribution is peaky). The blue voxel is the expert keyframe action. And the turquoise gripper (without fingers) is the 6-DoF action with the highest Q-values. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}